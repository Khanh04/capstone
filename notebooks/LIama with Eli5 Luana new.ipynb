{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac2902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c610819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43de3e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: Tesla V100-PCIE-16GB. Max memory: 15.766 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers, TRL and unsloth via:\n",
      "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048*2 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,)\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "baseline_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f9a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322b63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples['question']\n",
    "    answers = examples['answer']\n",
    "    convos = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        convos.append([{\"role\": \"user\", \"content\": question}, {\"role\": \"assistant\", \"content\": answer}])\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464178fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"sentence-transformers/eli5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc2fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Step 2: Split the 'train' part (80%) into 'train' (90% of 80%) and 'validation' (10% of 80%)\n",
    "train_valid_split = train_test_split['train'].train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "# Step 3: Update the DatasetDict to include 'train', 'validation', and 'test' sets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_valid_split['train'],  # 90% of 80% => 72% of the original dataset\n",
    "    'validation': train_valid_split['test'],  # 10% of 80% => 8% of the original dataset\n",
    "    'test': train_test_split['test']  # 20% of the original dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31a7696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 263634\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 29293\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 32548\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(dataset['train'])\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5303d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 3000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        eval_strategy=\"steps\",  # You can also use 'epoch' if preferred\n",
    "        eval_steps=250,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = trainer_stats.model\n",
    "output_dir = \"C:/Users/luan/Desktop/peft_weights\"\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c833ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "\n",
    "# Load the PEFT configuration\n",
    "peft_model_id = \"C:/Users/luan/Desktop/peft_weights\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# Load the tokenizer and processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbf1e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Identity()\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "model.to(device)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e064394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : The difference between a learning disability and an intellectual disability\n",
      "Model Output: The difference between a learning disability and an intellectual disability is the level of mental functioning. Intellectual disabilities are characterized by severe limitations in cognitive abilities, such as difficulty with language or memory. Learning disabilities refer to difficulties that affect one's ability to learn new information through various methods (e.g., reading, writing) without necessarily having any underlying problems with intelligence.\n",
      "\n",
      "Hereâ€™s some more specific examples:  * Dyslexia refers to a disorder which affects how you process written words into meaningful sentences and paragraphs; it can cause trouble understanding what someone has said if they donâ€™t read their own speech out loud while speaking. This may be due to difficulties processing phonemesâ€”small units within spoken soundsâ€”and/or semantic skillsâ€”the way your brain interprets meaning from those sound patterns. It doesn't mean anyone who learns this will never have good spelling or grammar skills, but rather challenges when trying to understand complex texts. * Dyscalculia is related to math and arithmetic functions, including numbers being difficult for people to count up high enough to perform basic addition/subtraction tasks accurately\n",
      "----------\n",
      "Base Model Output: The difference between a learning disability and an intellectual disability is how much the person's cognitive abilities are impaired. Learning disabilities affect specific skills or knowledge areas, while intellectual disabilities affect broader mental processes such as thought patterns and problem-solving ability.\n",
      "\n",
      "There are many different types of learning disabilities that can impact individuals in various ways. Some common examples include:\n",
      "\n",
      "* Dyslexia: Difficulty with reading comprehension\n",
      "* Attention Deficit Hyperactivity Disorder (ADHD): Trouble focusing on tasks for long periods of time, difficulty sustaining attention during activities\n",
      "* Autism Spectrum Disorders (ASD) : difficulties socializing and interacting appropriately with others, trouble understanding tone of voice and facial expressions.\n",
      "* Tourette Syndrome: Tics and other motor symptoms associated with this disorder\n",
      "\n",
      "Intellectual Disabilities occur when there is some level of brain damage to the development process itself. This may be due to genetic causes or environmental factors. The most well-known form of Intellectual Disability is Down syndrome but it also occurs sporadically without any known cause. Other forms of Intellectual Disability have been linked to infections like\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question=\"The difference between a learning disability and an intellectual disability\"\n",
    "inputs = tokenizer(text=question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "    base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "base_generated_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "print(f\"Question : {question}\")\n",
    "print(f\"Model Output: {generated_text}\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"Base Model Output: {base_generated_text}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8239d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4089934/3651912416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming your test dataset is a list called 'test_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msampled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Assuming your test dataset is a list called 'test_data'\n",
    "sampled_data = random.sample(dataset['test']['question'], 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5c19617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|â–Ž         | 1/35 [00:12<07:13, 12.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|â–Œ         | 2/35 [00:24<06:35, 11.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|â–Š         | 3/35 [00:36<06:34, 12.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|â–ˆâ–        | 4/35 [00:46<05:53, 11.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|â–ˆâ–        | 5/35 [00:59<05:56, 11.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|â–ˆâ–‹        | 6/35 [01:11<05:47, 11.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|â–ˆâ–ˆ        | 7/35 [01:21<05:17, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8/35 [01:30<04:45, 10.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 9/35 [01:43<04:52, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|â–ˆâ–ˆâ–Š       | 10/35 [01:56<04:53, 11.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [02:05<04:20, 10.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [02:17<04:22, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [02:29<04:13, 11.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [02:42<04:09, 11.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [02:52<03:46, 11.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [03:02<03:27, 10.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [03:12<03:11, 10.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [03:22<02:57, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [03:32<02:45, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [03:42<02:31, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [03:51<02:20, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [04:02<02:10, 10.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [04:11<02:00, 10.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [04:21<01:50, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [04:33<01:45, 10.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [04:43<01:33, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [04:53<01:22, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [05:02<01:08,  9.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [05:12<00:59,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [05:22<00:49,  9.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [05:32<00:39,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [05:42<00:29,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [05:55<00:21, 10.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [06:07<00:11, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:17<00:00, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 350 answers from the fine-tuned model.\n",
      "Generated 350 answers from the baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 10  # Start with a small batch size\n",
    "all_generated_texts = []\n",
    "all_base_generated_texts = []\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for i in tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    # Get the current batch of questions\n",
    "    batch_questions = sampled_data[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        # Tokenize and move to GPU\n",
    "        inputs = processor(text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "            base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "        generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "        base_generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in base_outputs]\n",
    "\n",
    "        # Collect the results\n",
    "        all_generated_texts.extend(generated_texts)\n",
    "        all_base_generated_texts.extend(base_generated_texts)\n",
    "\n",
    "        # Clear cache after each batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM error. Consider reducing batch size.\")\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"Generated {len(all_generated_texts)} answers from the fine-tuned model.\")\n",
    "print(f\"Generated {len(all_base_generated_texts)} answers from the baseline model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81bbeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "output_df = pd.DataFrame({\n",
    "    \"question\": sampled_data,\n",
    "    \"baseline_answer\": all_base_generated_texts,\n",
    "    \"fine_tuned_answer\": all_generated_texts\n",
    "})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df.to_csv(\"generated_answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed9428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df=pd.read_csv(\"generated_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f308de59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>baseline_answer</th>\n",
       "      <th>fine_tuned_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Has there ever been a point in time when the K...</td>\n",
       "      <td>Has there ever been a point in time when the K...</td>\n",
       "      <td>Has there ever been a point in time when the K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can NASA so accurately pinpoint the locati...</td>\n",
       "      <td>How can NASA so accurately pinpoint the locati...</td>\n",
       "      <td>How can NASA so accurately pinpoint the locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it true that Modern Man evolved in Africa?</td>\n",
       "      <td>Is it true that Modern Man evolved in Africa? ...</td>\n",
       "      <td>Is it true that Modern Man evolved in Africa? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can the wind all over the world just stop? If ...</td>\n",
       "      <td>Can the wind all over the world just stop? If ...</td>\n",
       "      <td>Can the wind all over the world just stop? If ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How is it possible that we feel heat from the ...</td>\n",
       "      <td>How is it possible that we feel heat from the ...</td>\n",
       "      <td>How is it possible that we feel heat from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Why do all countries have a rectangular flag? ...</td>\n",
       "      <td>Why do all countries have a rectangular flag? ...</td>\n",
       "      <td>Why do all countries have a rectangular flag? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Why did the Saxons call it England?</td>\n",
       "      <td>Why did the Saxons call it England? They were ...</td>\n",
       "      <td>Why did the Saxons call it England? The name \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Are there precious resources (ie gold, copper,...</td>\n",
       "      <td>Are there precious resources (ie gold, copper,...</td>\n",
       "      <td>Are there precious resources (ie gold, copper,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Can lorentz contractions and time dilation mak...</td>\n",
       "      <td>Can lorentz contractions and time dilation mak...</td>\n",
       "      <td>Can lorentz contractions and time dilation mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Difference between Anarchism and Communism</td>\n",
       "      <td>Difference between Anarchism and Communism. Th...</td>\n",
       "      <td>Difference between Anarchism and Communism\\nAn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Has there ever been a point in time when the K...   \n",
       "1    How can NASA so accurately pinpoint the locati...   \n",
       "2        Is it true that Modern Man evolved in Africa?   \n",
       "3    Can the wind all over the world just stop? If ...   \n",
       "4    How is it possible that we feel heat from the ...   \n",
       "..                                                 ...   \n",
       "345  Why do all countries have a rectangular flag? ...   \n",
       "346                Why did the Saxons call it England?   \n",
       "347  Are there precious resources (ie gold, copper,...   \n",
       "348  Can lorentz contractions and time dilation mak...   \n",
       "349         Difference between Anarchism and Communism   \n",
       "\n",
       "                                       baseline_answer  \\\n",
       "0    Has there ever been a point in time when the K...   \n",
       "1    How can NASA so accurately pinpoint the locati...   \n",
       "2    Is it true that Modern Man evolved in Africa? ...   \n",
       "3    Can the wind all over the world just stop? If ...   \n",
       "4    How is it possible that we feel heat from the ...   \n",
       "..                                                 ...   \n",
       "345  Why do all countries have a rectangular flag? ...   \n",
       "346  Why did the Saxons call it England? They were ...   \n",
       "347  Are there precious resources (ie gold, copper,...   \n",
       "348  Can lorentz contractions and time dilation mak...   \n",
       "349  Difference between Anarchism and Communism. Th...   \n",
       "\n",
       "                                     fine_tuned_answer  \n",
       "0    Has there ever been a point in time when the K...  \n",
       "1    How can NASA so accurately pinpoint the locati...  \n",
       "2    Is it true that Modern Man evolved in Africa? ...  \n",
       "3    Can the wind all over the world just stop? If ...  \n",
       "4    How is it possible that we feel heat from the ...  \n",
       "..                                                 ...  \n",
       "345  Why do all countries have a rectangular flag? ...  \n",
       "346  Why did the Saxons call it England? The name \"...  \n",
       "347  Are there precious resources (ie gold, copper,...  \n",
       "348  Can lorentz contractions and time dilation mak...  \n",
       "349  Difference between Anarchism and Communism\\nAn...  \n",
       "\n",
       "[350 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether LLAMA change its answering style due to fine tune by ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32001977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dataset' is your loaded DatasetDict\n",
    "# Example: dataset = DatasetDict.load_from_disk('path_to_your_dataset')\n",
    "\n",
    "# Extract the first 1000 QA pairs\n",
    "qa_pairs = output_df\n",
    "\n",
    "# Initialize lists to store lengths\n",
    "question_lengths = []\n",
    "answer_lengths = []\n",
    "\n",
    "# Calculate sentence lengths for each QA pair\n",
    "for pair in range(len(qa_pairs)):\n",
    "    question = qa_pairs['baseline_answer'][pair]\n",
    "    answer = qa_pairs['fine_tuned_answer'][pair]\n",
    "    \n",
    "    # Count sentences in question and answer\n",
    "    question_length = len(sent_tokenize(question))\n",
    "    answer_length = len(sent_tokenize(answer))\n",
    "    \n",
    "    # Append lengths to lists\n",
    "    question_lengths.append(question_length)\n",
    "    answer_lengths.append(answer_length)\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "lengths_df = pd.DataFrame({\n",
    "    'Question Length': question_lengths,\n",
    "    'Answer Length': answer_lengths\n",
    "})\n",
    "\n",
    "# Save lengths to a CSV file (optional)\n",
    "lengths_df.to_csv('qa_lengths.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Question lengths\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(question_lengths, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Baseline Answers Length Distribution')\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Answer lengths\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(answer_lengths, bins=30, color='green', alpha=0.7)\n",
    "plt.title('Fine_tuned Answers Length Distribution')\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50dabdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "question_sentence_lengths = []\n",
    "answer_sentence_lengths = []\n",
    "\n",
    "# Calculate sentence lengths for each QA pair\n",
    "for pair in range(len(qa_pairs)):\n",
    "    question = qa_pairs['baseline_answer'][pair]\n",
    "    answer = qa_pairs['fine_tuned_answer'][pair]\n",
    "    \n",
    "    # Tokenize sentences and calculate word counts for each sentence\n",
    "    question_sentences = sent_tokenize(question)\n",
    "    answer_sentences = sent_tokenize(answer)\n",
    "    \n",
    "    question_sentence_lengths.extend([len(word_tokenize(sentence)) for sentence in question_sentences])\n",
    "    answer_sentence_lengths.extend([len(word_tokenize(sentence)) for sentence in answer_sentences])\n",
    "\n",
    "# Visualize each distribution separately\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Question sentence lengths\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(question_sentence_lengths, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Question Sentence Length Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Answer sentence lengths\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(answer_sentence_lengths, bins=30, color='green', alpha=0.7)\n",
    "plt.title('Answer Sentence Length Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1656837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check technical jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054464e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>FREQcount</th>\n",
       "      <th>CDcount</th>\n",
       "      <th>FREQlow</th>\n",
       "      <th>Cdlow</th>\n",
       "      <th>SUBTLWF</th>\n",
       "      <th>Lg10WF</th>\n",
       "      <th>SUBTLCD</th>\n",
       "      <th>Lg10CD</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Freq_dom_PoS_SUBTLEX</th>\n",
       "      <th>Percentage_dom_PoS</th>\n",
       "      <th>All_PoS_SUBTLEX</th>\n",
       "      <th>All_freqs_SUBTLEX</th>\n",
       "      <th>Zipf-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1041179</td>\n",
       "      <td>8382</td>\n",
       "      <td>976941</td>\n",
       "      <td>8380</td>\n",
       "      <td>20415.274510</td>\n",
       "      <td>6.017526</td>\n",
       "      <td>99.928469</td>\n",
       "      <td>3.923399</td>\n",
       "      <td>Article</td>\n",
       "      <td>993445.0</td>\n",
       "      <td>0.960658</td>\n",
       "      <td>Article.Adverb.Letter.To.Noun.Preposition.Adje...</td>\n",
       "      <td>993445.33186.6441.744.257.52.5</td>\n",
       "      <td>7.309360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>87</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.705882</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>0.834526</td>\n",
       "      <td>1.851258</td>\n",
       "      <td>Name</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>Name.Noun</td>\n",
       "      <td>79.7</td>\n",
       "      <td>3.236317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaa</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>1.414973</td>\n",
       "      <td>0.274201</td>\n",
       "      <td>1.380211</td>\n",
       "      <td>Name</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Name.Noun</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.706807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aah</td>\n",
       "      <td>2688</td>\n",
       "      <td>634</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>52.705882</td>\n",
       "      <td>3.429591</td>\n",
       "      <td>7.558417</td>\n",
       "      <td>2.802774</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>2657.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>2657</td>\n",
       "      <td>4.721425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aahed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>Verb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Verb</td>\n",
       "      <td>1</td>\n",
       "      <td>1.592864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74281</th>\n",
       "      <td>zygoma</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>Noun</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Noun</td>\n",
       "      <td>2</td>\n",
       "      <td>1.768955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74282</th>\n",
       "      <td>zygomatic</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>5</td>\n",
       "      <td>2.069985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74283</th>\n",
       "      <td>zygote</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>Noun</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>Noun.Verb</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.194924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74284</th>\n",
       "      <td>zygotes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1</td>\n",
       "      <td>1.592864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74285</th>\n",
       "      <td>zymurgy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1</td>\n",
       "      <td>1.592864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74286 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  FREQcount  CDcount  FREQlow  Cdlow       SUBTLWF    Lg10WF  \\\n",
       "0              a    1041179     8382   976941   8380  20415.274510  6.017526   \n",
       "1             aa         87       70        6      5      1.705882  1.944483   \n",
       "2            aaa         25       23        5      3      0.490196  1.414973   \n",
       "3            aah       2688      634       52     37     52.705882  3.429591   \n",
       "4          aahed          1        1        1      1      0.019608  0.301030   \n",
       "...          ...        ...      ...      ...    ...           ...       ...   \n",
       "74281     zygoma          2        2        1      1      0.039216  0.477121   \n",
       "74282  zygomatic          5        5        5      5      0.098039  0.778151   \n",
       "74283     zygote          7        5        4      3      0.137255  0.903090   \n",
       "74284    zygotes          1        1        1      1      0.019608  0.301030   \n",
       "74285    zymurgy          1        1        1      1      0.019608  0.301030   \n",
       "\n",
       "         SUBTLCD    Lg10CD Dom_PoS_SUBTLEX  Freq_dom_PoS_SUBTLEX  \\\n",
       "0      99.928469  3.923399         Article              993445.0   \n",
       "1       0.834526  1.851258            Name                  79.0   \n",
       "2       0.274201  1.380211            Name                  20.0   \n",
       "3       7.558417  2.802774    Interjection                2657.0   \n",
       "4       0.011922  0.301030            Verb                   1.0   \n",
       "...          ...       ...             ...                   ...   \n",
       "74281   0.023844  0.477121            Noun                   2.0   \n",
       "74282   0.059609  0.778151       Adjective                   5.0   \n",
       "74283   0.059609  0.778151            Noun                   4.0   \n",
       "74284   0.011922  0.301030            Noun                   1.0   \n",
       "74285   0.011922  0.301030            Noun                   1.0   \n",
       "\n",
       "       Percentage_dom_PoS                                    All_PoS_SUBTLEX  \\\n",
       "0                0.960658  Article.Adverb.Letter.To.Noun.Preposition.Adje...   \n",
       "1                0.918605                                          Name.Noun   \n",
       "2                0.800000                                          Name.Noun   \n",
       "3                1.000000                                       Interjection   \n",
       "4                1.000000                                               Verb   \n",
       "...                   ...                                                ...   \n",
       "74281            1.000000                                               Noun   \n",
       "74282            1.000000                                          Adjective   \n",
       "74283            0.571429                                          Noun.Verb   \n",
       "74284            1.000000                                               Noun   \n",
       "74285            1.000000                                               Noun   \n",
       "\n",
       "                    All_freqs_SUBTLEX  Zipf-value  \n",
       "0      993445.33186.6441.744.257.52.5    7.309360  \n",
       "1                                79.7    3.236317  \n",
       "2                                20.5    2.706807  \n",
       "3                                2657    4.721425  \n",
       "4                                   1    1.592864  \n",
       "...                               ...         ...  \n",
       "74281                               2    1.768955  \n",
       "74282                               5    2.069985  \n",
       "74283                             4.3    2.194924  \n",
       "74284                               1    1.592864  \n",
       "74285                               1    1.592864  \n",
       "\n",
       "[74286 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the SUBTLEX dataset (replace 'path_to_subtlex.csv' with your file path)\n",
    "subtlex_data = pd.read_excel('SUBTLEX-US frequency list with PoS and Zipf information.xlsx')\n",
    "subtlex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7225681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Answer_Index  Average_Frequency\n",
      "0             0      283226.183099\n",
      "1             1      290468.239669\n",
      "2             2      192989.009901\n",
      "3             3      228029.388889\n",
      "4             4      232714.198068\n"
     ]
    }
   ],
   "source": [
    "answers=output_df['baseline_answer']\n",
    "\n",
    "#['fine_tuned_answer']\n",
    "def calculate_average_frequency(text, subtlex_data):\n",
    "    words = text.split()  # Split text into words\n",
    "    frequencies = []\n",
    "    for word in words:\n",
    "        # Match word in SUBTLEX\n",
    "        match = subtlex_data[subtlex_data['Word'].str.lower() == word.lower()]\n",
    "        if not match.empty:\n",
    "            frequencies.append(match.iloc[0]['FREQcount'])  # Append frequency if found\n",
    "        else:\n",
    "            frequencies.append(0)  # Use 0 if word not found in SUBTLEX\n",
    "    return sum(frequencies) / len(frequencies) if frequencies else 0\n",
    "\n",
    "results = []\n",
    "\n",
    "# Apply function to each answer and store index and average frequency\n",
    "for index, answer in enumerate(answers):\n",
    "    avg_freq = calculate_average_frequency(answer, subtlex_data)\n",
    "    results.append((index, avg_freq))  # Store index and average frequency as a tuple\n",
    "\n",
    "# Optionally convert results into a DataFrame\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Answer_Index', 'Average_Frequency'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('average_frequencies.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5926593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Plot frequencies\n",
    "average_frequency = results_df['Average_Frequency'].mean()\n",
    "\n",
    "plt.bar(range(len(results_df['Average_Frequency'])), results_df['Average_Frequency'])\n",
    "plt.axhline(y=average_frequency, color='red', linestyle='--', label=f'Average ({average_frequency:.2f})')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Answer Index')\n",
    "plt.ylabel('Average Word Frequency')\n",
    "plt.title('Average Word Frequency per Answer')\n",
    "plt.legend()  # Add legend to explain the average line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f4826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Answer_Index  Average_Frequency\n",
      "0             0      247687.263158\n",
      "1             1      318399.000000\n",
      "2             2      154858.824176\n",
      "3             3      219850.026667\n",
      "4             4      249186.459459\n"
     ]
    }
   ],
   "source": [
    "answers=output_df['fine_tuned_answer']\n",
    "\n",
    "#['fine_tuned_answer']\n",
    "def calculate_average_frequency(text, subtlex_data):\n",
    "    words = text.split()  # Split text into words\n",
    "    frequencies = []\n",
    "    for word in words:\n",
    "        # Match word in SUBTLEX\n",
    "        match = subtlex_data[subtlex_data['Word'].str.lower() == word.lower()]\n",
    "        if not match.empty:\n",
    "            frequencies.append(match.iloc[0]['FREQcount'])  # Append frequency if found\n",
    "        else:\n",
    "            frequencies.append(0)  # Use 0 if word not found in SUBTLEX\n",
    "    return sum(frequencies) / len(frequencies) if frequencies else 0\n",
    "\n",
    "results = []\n",
    "\n",
    "# Apply function to each answer and store index and average frequency\n",
    "for index, answer in enumerate(answers):\n",
    "    avg_freq = calculate_average_frequency(answer, subtlex_data)\n",
    "    results.append((index, avg_freq))  # Store index and average frequency as a tuple\n",
    "\n",
    "# Optionally convert results into a DataFrame\n",
    "\n",
    "results_df_tuned = pd.DataFrame(results, columns=['Answer_Index', 'Average_Frequency'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df_tuned.to_csv('tuned_average_frequencies.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df_tuned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8319514d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6XklEQVR4nO3deZgU5bn38e9P8QgqIiB6kEEhigZRQEDELaIo4HZwAUVjQCViDCaamBPx1SOJSxITt5gEIwYCGmVxJ66gCEYOuwGRRRkBwwiHVQUlIIP3+0c9PdQM3T09QE93z9yf66qrq5+q56m7qrvr7tplZjjnnHN72l65DsA551zN5AnGOedcVniCcc45lxWeYJxzzmWFJxjnnHNZ4QnGOedcVniCca4KJLWQZJLq5DoW5/KdJ5haStJkSZ9J2jfXsewOSU3DCv/QWNntKcper4Z4lkv6t6QvY91h2Z5ubSSppaRvJA3NdSwuOU8wtZCkFsDpgAH/lYX2q+3fvZmtAoqB78SKvwMsTlL2TlXa3o35uNDMDoh1K/dQu7VOJcuqH/AZ0Ddf/ygpUmvXs7V2xmu5fsB0YCTQH0DSvpI+l3RcYiRJTcK/8UPC+wskzQ3j/a+ktrFxl0u6VdL7wFeS6kgaLOljSZskLZR0cWz8vSU9IGmdpGWSbozvepLUQNJwSaskfSrpHkl7p5ifdwjJJIxzAvD7CmUnA+9I2kvSHZI+kbRG0hOSGoTxEru/Bkj6FzApxHl/iHMpcP6uLPDQ7iBJS4AlGSzPEyS9F5bdWEljJN0Thl0t6d0k7R8V+vcNMf9L0mpJf5ZULwzrKqlE0i1h/ldJuibWTr3wuXwi6QtJ74ayVyT9qMI035d0UZJ5TSzHgZJWhmncEhu+V+y7sV7SOEmNUn0GaRZrP+AOYBtwYZLl8QNJSxRtqf9JksKwoyRNCfO3TtLYUP5LSX8I/ftI+krSb2PLZYukhuF9l/CZfS5pnqSusWlPlnSvpKnAZuBbaeahZjMz72pZR/SP/4dAR6If56GhfARwb2y8QcDrob8DsAY4CdibKDEtB/YNw5cDc4HmQL1Q1gc4jOiPzOXAV0DTMOwHwEKgCGgIvEm0RVUnDH8ReAzYHzgEmAlcn2J++gPzQn8nooTTqkLZv4H/AK4N8/8t4ADgeeDJMF6LEMMTYbr1QpyLw3w1At6Ox5kkluXA2UnKDZgY2qiXbnmGOD8BfgLsA/QOn9M9oa2rgXeTtH9U6H8YGB+mVR/4O/DrMKwrUArcFdo+j2gl2DAM/xMwGWgW4jolxHQZMCM2vXbAeuA/ksxrYjmODsvxeGBtYrkANxP9wSkKbT8GjE71GaRYzqcDW4m+O38AxidZHi8DBwGHh+n3DMNGA7cTfS/rAqeF8rOA+aH/FODjxDyHYYnvU7Mw7+eFNs4J75uE4ZOBfwFtgDrAPrn+zedsXZPrALyr5g8cTgsrq4PD+8XAT0L/2cDS2LhTgX6h/1Hg7gptfQicEfqXA9dWMu25QK/QP4lYwgjTtvCDPDSsPOrFhl8BvJ2i3RbA9rCy+QkhSQKfxsreDmVvAT+M1T0mLI86sZXbt2LDJwE/iL3vTuUJ5kvg89C9GMoNOCs2XsrlSbTltRJQbNj/kkGCAUSUyI+MDTsZWBb6uxIl2zqx4WuALmFl+W+gXZL52hfYALQK7+8Hhqb5PAz4dqzst8Dw0L8I6BYb1jTdZ5BiGn+JLduTQ/1DKiyP02LvxwGDQ/8TwDCgqEKb9YAtQGNgMPD/gBKiPyK/BB4J491K+FMSq/sG0D/0TwbuysXvO98630VW+/QHJpjZuvD+6VAG0cq0nqSTJB0BtAdeCMOOAG4JuwQ+l/Q50b/6+AHsFfEJSeoX2wX0OXAccHAYfFiF8eP9RxD9u14Vq/sY0ZbMTsxsOdGK4DSilfM/wqBpsbLE8ZfDiLYOEj5hR1JLFkvFOON1U7nIzA4K3UUp2k23PA8DPrWwtqrCdAGaAPsBc2Ltvh7KE9abWWns/WailejBRP/oP67YqJltJVpJX6XomMIVwJOVxFJxuSW+K0cAL8TiW0T0ByHVZ1BO2N3XB3gqxDaNaIvhygqj/l+sPzGPAD8nSsQzJS2QdG1o59/AbHYk+SlEif3UUDYlFn+fCp/daUSJstL4axM/2FiLhB/mZcDekhI/vn2BgyS1M7N5ksYRrTxWAy+b2aYw3gqiLYN700yibIUYEtTjQDdgmpltlzSX6IcNsIpoF0lC81j/CqItmIMrrAjT+QfRSuFkdiTMRNlpwB9D2UqiFUTC4US7jFbH4omv2FdViO3wDONJJt5uyuUp6QygmSTFkszh7Fjxf0WURBLj/2es+jqirZA2ZvZpFeNbR/QP/khgXpLho4iSyrvA5rBiT6c50RZyIv7EyQ4riLZ2p1asoOgEFCi/rCq6GDgQGJo4ZkK0K6wf0e7BtMzs/4DrwvROA96U9I6ZFRMlkbOIjuPNCu97AJ3Z8SdlBdEWzHXpJlNZHLWBb8HULhcR/VM8lmjrpD3QmmhF3C+M8zTR8ZLvhv6Ex4EfhK0bSdpf0vmS6qeY1v5EP7K1AOFA8nGx4eOAmyQ1k3QQ0W4HoOzMsAnAA5IODAeFjwwr3lTeCfOw0sw2hrJ3Q1kDoq0ZiPa//0TRKa4HAL8CxqZJZOOAH0sqCgd4B6eJoSrSLc9pREnvx4pOlriEaAWXMA9oI6m9pLrALxIDzOyb0PZD2nFyRjNJPSoLKNQdATwo6TBFJzicrHCGVkgo3wAPUPnWC8D/SNpPUhvgGmBsKP8zcG/4E5I4maRXBu0l9A9xHs+O7/GpQHtJx1dWWVIfSYk/E58RfU+3h/dTiL4zC83sa6LdXd8n2sW4NozzN+BCST3CMqqr6OSJ+B8mhyeY2qY/8Fcz+5eZ/V+iI/p3/11JdcxsBtE/5MOA1xIVzWw20b++PxL9KIuJjgUkZWYLiVZE04i2Do4nOqaT8DhREnkf+CfwKtFKNfFD70d0sHthmN6zlN8FUdEUol1o8bOr5hLtV59jZptD2QiileM7wDKif+zlzo6q4HGi/evzgPeITgrYbemWZ1ixXRLef0aU8J+P1f2I6CD9m0RnpJU7o4woWRcD0yVtDOMdk2FoPwPmE/173wDcR/n1xBNEn+XfMmhrSojjLeB+M5sQyn9PdBLCBEmbiA74n5RJcJKaEW0VPxz/DpvZHKJdgf3TtwDAicAMSV+GOG4ys2Vh2P8SfWcSWysLib4jZae4m9kKoBfRMZq1RFs0/42vT3ei8rt5ncsNSecCfzazIyoduRaSNBIoMbM7chxHP2CgmZ2WZpwWRMl7nyrs4nQ1kGdclxPhuoLzwi6gZsAQdpxQ4PKQpP2ITm8flutYXGHwBONyRUSnfn5GtItsEXBnTiNyKYVjOGuJdnc+XcnozgG+i8w551yW+BaMc865rPDrYIKDDz7YWrRokeswnHOuoMyZM2edmTVJNswTTNCiRQtmz56d6zCcc66gSEp5lwnfReaccy4rPME455zLCk8wzjnnssKPwThXC23bto2SkhK2bNmS61Bcgahbty5FRUXss88+GdfxBONcLVRSUkL9+vVp0aIFkiqv4Go1M2P9+vWUlJTQsmXLjOv5LjLnaqEtW7bQuHFjTy4uI5Jo3Lhxlbd4PcE4V0t5cnFVsSvfF08wzjnnssITjHMuZ1544QUksXjx4spHzjEz46yzzmLjxo2sWLGCM888k9atW9OmTRt+//vfl433zDPP0KZNG/baa69yF29//fXXXHPNNRx//PG0a9eOyZMnA7Bp0ybat29f1h188MHcfPPNAHzyySd069aNtm3b0rVrV0pKSsra+/nPf06bNm1o3bo1P/7xj0l2X8mRI0fSpEmTsrb/8pe/APD222+Xm2bdunV58cUXAejbty9LlizZcwvNO6Njx47m9qwjbn051yG4FBYuXJjrEMzMrE+fPnbaaafZkCFD9kh7paWle6SdZF5++WW7+eabzcxs5cqVNmfOHDMz27hxo7Vq1coWLFhgZtGyXbx4sZ1xxhk2a9assvp//OMf7eqrrzYzs9WrV1uHDh1s+/btO02nQ4cONmXKFDMz6927t40cOdLMzN566y276qqrzMxs6tSpdsopp1hpaamVlpZaly5d7O23396prb/+9a82aNCgtPO1fv16a9iwoX311VdmZjZ58mT7/ve/n3TcZN8bYLalWK/6FoxzLie+/PJLpk6dyvDhwxkzZgwAr732GpdddlnZOJMnT+bCCy8EYMKECZx88sl06NCBPn368OWXXwLRbZ7uuusuTjvtNJ555hkef/xxTjzxRNq1a8ell17K5s3Rw0w//vhjunTpwoknnsidd97JAQccUDad3/3ud5x44om0bduWIUOGJI33qaeeolev6MnOTZs2pUOHDgDUr1+f1q1b8+mnnwLQunVrjjlm5weILly4kG7dugFwyCGHcNBBB+10e6olS5awZs0aTj/99J3qnHnmmbz00ktAdDxky5YtfP3112zdupVt27Zx6KGHZrbgK3j22Wc599xz2W+//QA4/fTTefPNNykt3f1nxXmCcc5B1647d0OHRsM2b04+fOTIaPi6dTsPy8CLL75Iz549Ofroo2nUqBHvvfce55xzDtOnT+err74CYOzYsVx++eWsW7eOe+65hzfffJP33nuPTp068eCDD5a1VbduXd5991369u3LJZdcwqxZs5g3bx6tW7dm+PDhANx0003cdNNNzJo1i8MOO6ys7oQJE1iyZAkzZ85k7ty5zJkzh3feeYeKpk6dSseOHXcqX758Of/85z856aT0T31u164dL730EqWlpSxbtow5c+awYsWKcuOMHj2ayy+/vOyAert27XjuueeAaHfipk2bWL9+PSeffDJnnnkmTZs2pWnTpvTo0YPWrVsnne5zzz1H27Zt6d27907TAxgzZgxXXHFF2fu99tqLo446innz5qWdn0x4gnHO5cTo0aPp27cvEO33Hz16NHXq1KFnz578/e9/p7S0lFdeeYVevXoxffp0Fi5cyKmnnkr79u0ZNWoUn3yy4x6Ll19+eVn/Bx98wOmnn87xxx/PU089xYIFCwCYNm0affr0AeDKK68sG3/ChAlMmDCBE044gQ4dOrB48eKkxyA2bNhA/fr1y5V9+eWXXHrppTz88MMceOCBaef32muvpaioiE6dOnHzzTdzyimnUKdO+UsRK67s77//fqZMmcIJJ5zAlClTaNasGXXq1KG4uJhFixZRUlLCp59+yqRJk5ImxQsvvJDly5fz/vvvc/bZZ9O/f/9yw1etWsX8+fPp0aNHufJDDjmElStXpp2fTPiFls45CAeck9pvv/TDDz44/fAk1q9fz6RJk/jggw+QxPbt25HEb3/7Wy6//HL+9Kc/0ahRI0488UTq16+PmXHOOecwevTopO3tv//+Zf1XX301L774Iu3atWPkyJFlB9NTMTNuu+02rr/++rTj1alTh2+++Ya99or+l2/bto1LL72U7373u1xyySWVznOdOnV46KGHyt6fcsoptGrVquz9vHnzKC0tLbeVdNhhh/H8888DUTJ77rnnaNCgAcOGDaNLly5lu/nOPfdcpk+fzne+851y02zcuHFZ/3XXXcett95abvi4ceO4+OKLd7o6f8uWLdSrV6/SeaqMb8E456rds88+S79+/fjkk09Yvnw5K1asoGXLlrz77rt07dqV9957j8cff7xsy6RLly5MnTqV4uJiADZv3sxHH32UtO1NmzbRtGlTtm3bxlNPPVVW3qVLl7LdTYljPgA9evRgxIgRZcd0Pv30U9asWbNTu8cccwxLly4FoqQ0YMAAWrduzU9/+tOM5nnz5s1lu/4mTpxInTp1OPbYY8uGjx49utzWC8C6dev45ptvAPj1r3/NtddeC8Dhhx/OlClTKC0tZdu2bUyZMiXpLrJVq1aV9Y8fP36ncZJNE+Cjjz6iTZs2Gc1XOp5gnHPVbvTo0Vx88cXlyi699FKefvpp9t57by644AJee+01LrjgAgCaNGnCyJEjueKKK2jbti1dunRJeWrz3XffzUknncQ555zDt7/97bLyhx9+mAcffJDOnTuzatUqGjRoAED37t258sorOfnkkzn++OPp3bs3mzZt2qnd888/v2xraOrUqTz55JNMmjSp7FTfV199FYiOlRQVFTFt2jTOP//8st1Pa9asoUOHDrRu3Zr77ruPJ598slz748aN22llP3nyZI455hiOPvpoVq9eze233w5A7969OfLII8tOeW7Xrl3ZyRB33nkn48ePB+CRRx6hTZs2tGvXjkceeYSRieNmUJbYzzjjjHLTXL16NfXq1aNp06ZJl29VyJKcO10bderUyfyBY3tWi8GvsPw35+c6DJfEokWLUh4Urqk2b95MvXr1kMSYMWMYPXp02VlZmVi1ahX9+vVj4sSJWYwy9x566CEOPPBABgwYsNOwZN8bSXPMrFOytrK+BSNpb0n/lPRyeN9I0kRJS8Jrw9i4t0kqlvShpB6x8o6S5odhjyicYiFpX0ljQ/kMSS1idfqHaSyRVP7IlnOu1pkzZw7t27enbdu2DB06lAceeKBK9Zs2bcp1113Hxo0bsxRhfjjooIN2OhlgV1XHQf6bgEVA4hSLwcBbZvYbSYPD+1slHQv0BdoAhwFvSjrazLYDjwIDgenAq0BP4DVgAPCZmR0lqS9wH3C5pEbAEKATYMAcSePN7LNqmF/nXB46/fTTd/vU2/g1OjXVNddcs8fayuoWjKQi4HzgL7HiXsCo0D8KuChWPsbMtprZMqAY6CypKXCgmU0LV40+UaFOoq1ngW5h66YHMNHMNoSkMpEoKTnnAt897qpiV74v2d5F9jDwc+CbWNmhZrYKILweEsqbAfGrgEpCWbPQX7G8XB0zKwW+ABqnaascSQMlzZY0e+3atbswe84Vprp167J+/XpPMi4jFp4HU7du3SrVy9ouMkkXAGvMbI6krplUSVJmacp3tc6OArNhwDCIDvJnEKNzNUJRURElJSX4HyuXqcQTLasim8dgTgX+S9J5QF3gQEl/A1ZLampmq8Lur8QJ5yVA81j9ImBlKC9KUh6vUyKpDtAA2BDKu1aoM3nPzZpzhW2fffap0pMJndsVWdtFZma3mVmRmbUgOng/ycyuAsYDiVMU+gOJ8wTHA33DmWEtgVbAzLAbbZOkLuH4Sr8KdRJt9Q7TMOANoLukhuEste6hzDnnXDXJxa1ifgOMkzQA+BfQB8DMFkgaBywESoFB4QwygBuAkUA9orPHXgvlw4EnJRUTbbn0DW1tkHQ3MCuMd5eZbcj2jDnnnNuhWhKMmU0m7KIys/VAtxTj3Qvcm6R8NnBckvIthASVZNgIYMSuxuycc273+K1inHPOZYUnGOecc1nhCcY551xWeIJxzjmXFZ5gnHPOZYUnGOecc1nhCcY551xWeIJxzjmXFZ5g8kyLwa/kOgTnnNsjPME455zLCk8wzjnnssITjHPOuazwBONcDeXH81yueYJxzjmXFZ5gnHPOZUXWEoykupJmSponaYGkX4byX0j6VNLc0J0Xq3ObpGJJH0rqESvvKGl+GPZIeLIl4emXY0P5DEktYnX6S1oSuv4455yrVtl84NhW4Cwz+1LSPsC7khJPonzIzO6PjyzpWKInUrYBDgPelHR0eKrlo8BAYDrwKtCT6KmWA4DPzOwoSX2B+4DLJTUChgCdAAPmSBpvZp9lcX6dc87FZG0LxiJfhrf7hM7SVOkFjDGzrWa2DCgGOktqChxoZtPMzIAngItidUaF/meBbmHrpgcw0cw2hKQykSgpOeecqyZZPQYjaW9Jc4E1RCv8GWHQjZLelzRCUsNQ1gxYEateEsqahf6K5eXqmFkp8AXQOE1bzjnnqklWE4yZbTez9kAR0dbIcUS7u44E2gOrgAfC6ErWRJryXa1TRtJASbMlzV67dm2aOXHOOVdVlSaYcDxjt5jZ58BkoKeZrQ6J5xvgcaBzGK0EaB6rVgSsDOVFScrL1ZFUB2gAbEjTVsW4hplZJzPr1KRJk92ZReeccxVksgUzQ9Izks5LnL2VCUlNJB0U+usBZwOLwzGVhIuBD0L/eKBvODOsJdAKmGlmq4BNkrqE6fcDXorVSZwh1huYFI7TvAF0l9Qw7ILrHsqcc85Vk0zOIjuaKDlcC/xB0lhgpJl9VEm9psAoSXsTJbJxZvaypCcltSfaZbUcuB7AzBZIGgcsBEqBQeEMMoAbgJFAPaKzxxJnow0HnpRUTLTl0je0tUHS3cCsMN5dZrYhg3l1zjm3h1SaYMIWwURgoqQzgb8BP5Q0DxhsZtNS1HsfOCFJ+ffSTOte4N4k5bOB45KUbwH6pGhrBDAi1bQKRYvBr7D8N+fnOgznnKuyShOMpMbAVcD3gNXAj4h2TbUHngFaZjE+55xzBSqTXWTTgCeBi8wsfrrwbEl/zk5YzjnnCl0mCeaYsJtsJ2Z23x6OxznnXA2RyVlkExJngwGEM7P8jCyXVX6reecKXyYJpkm4jgWAcOuVQ7IWkXPO5Yj/sdmzMkkw2yUdnngj6QjS31PMOeecy+gYzO1Ed0KeEt5/h+jOxs4551xKmVwH87qkDkAXont8/cTM1mU9MueccwUt0+fB7Et0pXwd4FhJmNk72QvLOedcocvkQsv7gMuBBcA3odgATzDOOedSymQL5iKia2G2ZjkW55xzNUgmZ5EtJXoapXPOOZexTLZgNgNzJb0FlG3FmNmPsxaVc865gpdJghkfOueccy5jle4iM7NRwDhgupmNSnTZD80555LbnSvu/Wr96pPJI5MvBOYCr4f37SX5Fo1zzrm0MjnI/wugM/A5gJnNJYNnwEiqK2mmpHmSFkj6ZShvJGmipCXhtWGszm2SiiV9KKlHrLyjpPlh2COJRzeHxyuPDeUzJLWI1ekfprFEUn+ccwXPtz4KSyYJptTMvqhQlsm9yLYCZ5lZO6KHk/WU1AUYDLxlZq2At8J7JB1L9MjjNkBPYGh43DLAo0S3p2kVup6hfADwmZkdBTwE3BfaagQMAU4iSo5D4onMOedc9mWSYD6QdCWwt6RWkv4A/G9llSzyZXi7T+gM6AUkjuGMIrrOhlA+xsy2mtkyoBjoLKkpcKCZTQvPpXmiQp1EW88C3cLWTQ9gopltCHd/nsiOpOScc64aZJJgfkS0VbEVGA1sBG7OpHFJe0uaC6whWuHPAA41s1UA4TVx6/9mwIpY9ZJQ1iz0VywvV8fMSoEvgMZp2qoY30BJsyXNXrt2bSaz5JxzLkOZ3OxyM9EdlW+vauNmth1oHx5Y9oKk49KMrmRNpCnf1Trx+IYBwwA6derkjyBwzrk9KJOzyN6WNKliV5WJhAeWTSbaTbU67PYivK4Jo5UAzWPVioCVobwoSXm5OpLqAA2IbsqZqi3nnKtUPp9MkM+xVZTJLrKfAf8duv8hOmV5dmWVJDVJPGpZUj3gbGAx0UWbibO6+gMvhf7xQN9wZlhLooP5M8NutE2SuoTjK/0q1Em01RuYFI7TvAF0D493bgh0D2XOOeeqSSa7yOZUKJoae/hYOk2BUeFMsL2AcWb2sqRpwDhJA4B/AX3CdBZIGgcsBEqBQWEXG8ANwEigHvBa6ACGA09KKibacukb2tog6W5gVhjvLjPbkEHMzjnn9pBMbtffKPZ2L6Aj8J+V1TOz94ETkpSvB7qlqHMvcG+S8tnATsdvzGwLIUElGTYCGFFZnM4557Ijk11kc4h2ic0BpgG3EF1/4ly1K6T9z84/r9ouk11klV6171yutRj8Cst/c36uw3DOxWSyi+ySdMPN7Pk9F47bVb6Cdc7lm0xu1z8AOAVInJp8JtEpx18QXVviCWYP8d0JzrmaJJNjMAYca2aXmtmlRFf1Y2bXmNm1WY3OuVrG/2TkD/8sdl8mCaZF4tYuwWrg6CzF41xB85WS8+/ADpkkmMmS3pB0dbjt/SvA21mOy7lq5SsFl2s18TuYyRMtbwT+DCRuuz/MzH6U5bicy5ma+EMvZP55FK5MtmAA3gNeMbOfAG9Iqp/FmGok/5E452qbTG52eR3Rs1YeC0XNgBezGJNzzrkaIJMtmEHAqUTPgcHMlrDjGS6uFvKtMedcJjJJMFvN7OvEm3BbfH92Sg3gicI5l02ZJJgpkv4fUE/SOcAzwN+zG5arCk8UzhWG2vZbzSTB3AqsBeYD1wOvAndkMyiXG7Xty+9cTZRPv+O0CUbSXsB8M3vczPqYWe/Q77vIXEHY3R9btn+s+bQycG5PS5tgzOwbYJ6kw6vasKTm4XHLiyQtkHRTKP+FpE8lzQ3debE6t0kqlvShpB6x8o6S5odhj4QnWxKefjk2lM+Q1CJWp7+kJaHrTw2TyYrJV17OuVzKZBdZU2CBpLckjU90GdQrBW4xs9ZAF2CQpGPDsIfMrH3oXgUIw/oS3eusJzA0PA0T4FFgINFjlFuF4RDdiPMzMzsKeAi4L7TVCBgCnAR0BoaERyc7B3jydZkp1O9JvsSdMsGEs8UAfglcANwFPBDr0jKzVWb2XujfBCwiuoYmlV7AGDPbambLgGKgs6SmwIFmNi3smnsCuChWZ1TofxboFrZuegATzWyDmX0GTGRHUqp2+fJh76rqir/Ql5OrHv49KRzptmBmApjZFKC3mU2Jd1WZSNh1dQIwIxTdKOl9SSNiWxbNgBWxaiWhrFnor1hero6ZlRI9QqBxmrYqxjVQ0mxJs9euXVuVWXK1XL6v5HY1vnydr3yNy6WXLsEo1n/qrk5A0gHAc8DNZraRaHfXkUT3NVvFjq0hJaluacp3tc6OArNhZtbJzDo1adIk3WxUSU38MdTEedoTCmW5FEqctVFN/mzSJZjdPlNM0j5EyeWpxJMvzWy1mW0PJxA8TnSMBKKtjOax6kXAylBelKS8XJ2wS68BsCFNW87tlnxfGezp+PJ9fl1+S5dgvh12Y82P9b8fzuZ6v7KGw7GQ4cAiM3swVt40NtrFwAehfzzQN5wZ1pLoYP7M8CyaTZK6hDb7AS/F6iTOEOsNTArHad4AuktqGHbBdQ9lrppV5wqqsmn5yjJSE5dDTZynmiBdgmkNXEh0gD/Rn3h/YQZtnwp8DzirwinJv40lqTOBnwCY2QJgHLAQeB0YZGbbQ1s3AH8hOvD/MfBaKB8ONJZUDPwUGBza2gDcDcwK3V2hLO/4D6Nmqk2fa4vBr9Sq+c0HhXKZQp1UA8zsk91p2MzeJfmxkFfT1LkXuDdJ+WzguCTlW4A+KdoaAYzINN5C0GLwKyz/zfm5DqPWyocfrHOFJNPnwbhaxFekzuWXQv1NeoJxzmWsUFd0Ljc8wdRAfrC7evhyLHy1/TPM9vynu5J/fuzMsZ26rEblapza/kOuTKEeKI/HXIjxV6dsXPxacVhVxq0O6bZgEmeLvR6674buVaLbsrgqqmk/wJo2Py77/DtTu6RMMGb2STiT7FQz+7mZzQ/dYKJ7fblq5D/MmsM/yx0KaVkUUqz5IpNjMPtLOi3xRtIpwP7ZC8k552qGfH8eUbZlkmCuBf4kabmkZcDQUOZqkEL/IieTmKd8nbdcxJWvyyKfZLqMko23J5dvNtqq7s8/5YWWAOF5LGeYWTtJBwIysy+qJ7Rq9uGH0LVr+bLLLoMf/hA2b4bzztu5ztVXR926ddC7d1nxmKXrYfrv4IYb4PLLabpxLQ+9HO7pOf13O+rfcguwVzTt66+P6sXHuSM8mXruXMY8PbjcsDFL18N/NYRTTqFDySJ+/s6o8sOm/45jW14a1XnzTbjnnnLxTX96MN/qcWNU8Pe/M+bp/1fWfln8Tz4JzZtzwaJ3uOqfr+7UfsO2A6M6I0dGXQV1O/6ILfvUhaFDGfP0o+XnHWDy5Oj1/vvh5Zd3TBcY+a8vIXFR6d13w1tvlZv2a6u2ce6HU6Pht93GmKdfLr/sioqg6AoA7nxzGHT9Xbn2f/V53R3tDxzImNenlZ+/LRPh4Yej4VddBSUl5T6fn29twm/PuDp6f+mlsH59ufh+tL0ICO2fey78+9/llj8HLyK6QQbQteuOthPzkOa7N33perrcdUva794FDU7i5dbfgRUr4HvfKytPTGdA8QyGP3tX2XevYn3uuAPOPptjVy8t97soG/6rX5X/7sVjh2jZtW/PqcvnQtff7Vz/scfgmGPoVjyjbPj0pevp8q3G0Yjhu8fYsYx5+p6ythPx33DRbVFZ+O7FP1sAXn0V9tsPhg6FcePKLb8xS9eXffbXzXh+p/i21NmXqy/7JQA/mjq6bHiq7x7TppWrv6r+wWXtJ757ZfUBNrwEw4ZF/bHvXtk8tG9f9t176O/303TTunLLNv7de/SFX5Wb7zFL1zP1iHb84dTou5/47pX7fh28CH72s2j8pwfv/LvcxfVeMpU90XI70TNXMLONNTa57EHT4x+kS8v/Tad2x4sfVD6Sc/nOzNJ2RLdu+SNwOtAh0VVWr9C6jh072p5wxK0v2xG3vpy0rGJ5YlhldZMNS1cvXiddjKnaSBV/pu1n0maq6WQSSyYxxsuS1UnVbmX9FWPMNL5M5rOy5VVZfKnaTlae7rtR1fe7upwrG7fiNNLFX9l3MdNpVpyfdN/7VN/pTH+nqdpNFUuydqpaZ3eWWyrAbEuxXs3kGMwpRI8xjj/R8v5sJDtX8+VqqyXb+8uz0V5tt6cOkNeWzyUf57PSBGNmZybpzqqO4Nzuy8cvXU1UGy44rKnzVdtl83OtNMFIaiDpwcSjhSU9IKlB1iJyGfMffMSXg8t3tfU7mskushHAJuCy0G0E/prNoGq7Qt11U6g/Il8+e15tmleXWiYJ5kgzG2JmS0P3S+Bb2Q6spvMfoKtMbTuG4KqmEL4XmSSYf1e4kv9U4N9pxk+M11zS25IWSVog6aZQ3kjSRElLwmvDWJ3bJBVL+lBSj1h5x3DzzWJJj4RHJxMerzw2lM+Q1CJWp3+YxhJJ/almhfDhu8JVCN+vQogxW2rzvMdlkmBuYMeV/J8QnbJ8fSV1AEqBW8ysNdAFGCTpWKLHGr9lZq2At8J7wrC+RGes9QSGhgs9AR4FBgKtQtczlA8APjOzo4CHgPtCW42AIcBJQGdgSDyR5VJVvniF8CXdlRjzab7y7e6ztZXf1aBmSne7/ocl9QHWmlk7oC1wvJmdYGaV3q7fzFaZ2XuhfxOwCGhGdOFm4tLfUcBFob8XMMbMtprZMqAY6CypKXCgmU0L51w/UaFOoq1ngW5h66YHMNHMNpjZZ8BEdiQl5/KKr+h2Vh2nle+OfIoln6XbgikGLgamSloO/Bn4nqQTJFXpQWVh19UJwAzgUDNbBVESAg4JozUDVsSqlYSyZqG/Ynm5OmZWCnwBNE7TVsW4BibOjlu7dm1VZmmP8y+sc66mSXe7/j+a2ZVm1gI4GXgeOJJoS+HzTCcg6QDgOeBmM9uYbtRkYaQp39U6OwrMhplZJzPr1KRJkzShObd78uEPxJ580umemJ98WCa5VBvmP+2WiCJtiXZF9QLOAJYQXc1fKUn7ECWXp8zs+VC8Ouz2IryuCeUlQPNY9SJgZSgvSlJero6kOkADYEOatmqc6vyS5voH4WdV1Xz+2dYs6Y7BTASmA4OArcCvzKyjmfUMpyqnFY6FDAcWmdmDsUHjgcRZXf2Bl2LlfcOZYS2JDubPDLvRNknqEtrsV6FOoq3ewKRwnOYNoLukhuHgfvdQlhdqw4+oNsyjcy69dFswS4l2KyXO3DpK0sFVaPtU4HvAWZLmhu484DfAOZKWAOeE95jZAmAcsJDoEc2DLLqbM0Rnsv2F6LjQx8BroXw40FhSMfBTwhlpZrYBuBuYFbq7QlmtUZ0POqptyaSq81vou5Nq+uebb/NXk85sTPk8GDO7HiA8B6YL0U0vB0lqAnxgZmmvLTGzd0l+LASgW4o69xLdvbli+WzguCTlW4A+KdoaQXQXAlegsvFjajH4FZYnngNTQ9TEeapuvgyzI5OzwbYCm4kurtxKdDyjQzaDcs5Vn11N5Pn2b3pPnsSQzTZqk3THYB6SNANYRXSr/vrAY8AxZnZ8NcXnCkQ+/fDyKRaXnH9GtUO6LZhlRAf4m5jZWWZ2h5m9amafV09otUdtv9liPt7qPl/iqI182dcc6Y7BPFKdgTjnXLZ40sqNKl2R72q+ij9E/2E653aVJxhXEDzR7XnVtWuypn92NX3+dkdGCUbSaZKuCf1NwoWQzrlq5CsyV2gyeWTyEOBW4LZQtA/wt2wG5cor9BVLvt8ZtzKFFGtFhRz7rsjV/Bbacq6ueDPZgrkY+C/gKwAzW0l0yrJzeafQfuiVqWnzky98uVaPTBLM1+H+XgYgaf/shuScc7VbTUmAmSSYcZIeAw6SdB3wJvB4dsNyzu2qmrJyqg1q+meV8jqYBDO7X9I5wEbgGOBOM5uY9cicc2431fQVeL6rNMEAhITiScW5WsBv/Oj2lEoTjKRN7Pw0yC+A2cAtZrY0G4G5/OT/CJ1zmcpkC+ZBoqdBPk10+/2+wH8CHxLdDr9rtoJzNV9tS1i1bX5dZmrq9yKTg/w9zewxM9tkZhvNbBhwnpmNBRqmqiRphKQ1kj6Ilf1C0qcVHkCWGHabpGJJH0rqESvvKGl+GPZIeKol4cmXY0P5DEktYnX6S1oSurTPrXGuUNXUlZLLrur83mSSYL6RdJmkvUJ3WWxYxV1ncSOBnknKHzKz9qF7FUDSsURbRm1CnaGS9g7jPwoMZMeTNRNtDgA+M7OjgIeA+0JbjYAhwElAZ2BIeGxy1hXCfbzyMSbnarLa/JvLJMF8l+jRx2uA1aH/Kkn1gBtTVTKzd4BMH1PcCxhjZlvNbBnRo5E7S2oKHGhm08K1OE8AF8XqjAr9zwLdwtZND2CimW0ws8+ITk5IluicczVAbV6B57tMTlNeClyYYvC7uzDNGyX1Y8dJAp8BzYDpsXFKQtm20F+xnPC6IsRYKukLoHG8PEmdciQNJNo64vDDD9+FWXHO5ZInl/yWyb3I6koaJGloOK4yQtKuPuv+UeBIoD3RkzIfSEwmybiWpnxX65QvNBtmZp3MrFOTJk3ShF04/Afn8l1N/Y7W1PnaHZnsInuS6KyxHsAUoAjYtCsTM7PVZrbdzL4huhtA5zCoBGgeG7WI6My1ktBfsbxcHUl1gAZEu+RSteWcc64aZZJgjjKz/wG+MrNRwPnA8bsysXBMJeFiIHGG2XigbzgzrCXRwfyZZrYK2CSpSzi+0g94KVYncYZYb2BSOE7zBtBdUsNwcL97KHPOOVeNMrkOZlt4/VzSccD/AS0qqyRpNNE1MgdLKiE6s6urpPZEu6yWA9cDmNkCSeOAhUApMMjMtoembiA6I60e8FroAIYDT0oqJtpy6Rva2iDpbmBWGO8uM8v0ZAPnnHN7SCYJZljYEriDaKvhAOB/KqtkZlckKR6eZvx7gXuTlM8GjktSvgXok6KtEUQXgTrnnMuRtAlG0l7AxnCm1zvAt6olKueccwUv7TGYcDA+5bUuzjnnXCqZHOSfKOlnkppLapTosh6Zc865gpbJMZhrw+ugWJnhu8ucc86lkcmV/C2rIxDnnHM1SyZX8u8n6Q5Jw8L7VpIuyH5ozrl84Vepu12RyTGYvwJfA6eE9yXAPVmLyDnnXI2QSYI50sx+S7jg0sz+TfL7fTnnnHNlMkkwX4db8xuApCOBrVmNyjnnXMHL5CyyXwCvA80lPQWcClydxZicc87VAJmcRTZB0hygC9GusZvMbF3WI3POOVfQKk0wksYDo4HxZvZV9kNyzjlXE2RyDOYB4HRgoaRnJPWWVDfLcTnnnCtwmewimwJMkbQ3cBZwHdGdig/McmzOOecKWCYH+QlnkV0IXA50AEZlMyjnnHOFL5NjMGOBk4jOJPsTMDncZdk555xLKdMr+Y80sx+Y2STgZEl/qqySpBGS1kj6IFbWSNJESUvCa8PYsNskFUv6UFKPWHlHSfPDsEfCo5MJj1ceG8pnSGoRq9M/TGOJpMRjlZ1zzlWjShOMmb0OHC/pPknLiW4TsziDtkcCPSuUDQbeMrNWwFvhPZKOJXrkcZtQZ2g45gPwKDAQaBW6RJsDgM/M7CjgIeC+0FYjoscznwR0BobEE5lzzrnqkTLBSDpa0p2SFgF/JLoHmczsTDP7Q2UNm9k7wIYKxb3YcfxmFHBRrHyMmW01s2VAMdBZUlPgQDObZmYGPFGhTqKtZ4FuYeumBzDRzDaEJ3FOZOdE55xzLsvSbcEsBroBF5rZaSGpbN/N6R1qZqsAwushobwZsCI2Xkkoaxb6K5aXq2NmpcAXQOM0be1E0kBJsyXNXrt27W7MlnPOuYrSJZhLgf8D3pb0uKRuZO8ml8natTTlu1qnfKHZMDPrZGadmjRpklGgzjnnMpMywZjZC2Z2OfBtYDLwE+BQSY9K6r6L01sddnsRXteE8hKgeWy8ImBlKC9KUl6ujqQ6QAOiXXKp2nLOOVeNMjnI/5WZPWVmFxCtrOcSDs7vgvFA4qyu/sBLsfK+4cywlkQH82eG3WibJHUJx1f6VaiTaKs3MCkcp3kD6C6pYTi43z2UOeecq0YZXWiZYGYbgMdCl5ak0UBX4GBJJURndv0GGCdpAPAvoE9od4GkccBCoBQYZGaJ4z03EJ2RVg94LXQAw4EnJRUTbbn0TcQo6W5gVhjvrhC3c865alSlBFMVZnZFikHdUox/L3BvkvLZwHFJyrcQElSSYSOIbmfjnHMuRzK50NI555yrMk8wzjnnssITjHPOuazwBOOccy4rPME455zLCk8wzjnnssITjHPOuazwBOOccy4rPME455zLCk8wzjnnssITjHPOuazwBOOccy4rPME455zLCk8wzjnnssITjHPOuazISYKRtFzSfElzJc0OZY0kTZS0JLw2jI1/m6RiSR9K6hEr7xjaKZb0SHjqJeHJmGND+QxJLap9Jp1zrpbL5RbMmWbW3sw6hfeDgbfMrBXwVniPpGOJnlbZBugJDJW0d6jzKDCQ6BHLrcJwgAHAZ2Z2FPAQcF81zI9zzrmYfNpF1gsYFfpHARfFyseY2VYzWwYUA50lNQUONLNpZmbAExXqJNp6FuiW2LpxzjlXPXKVYAyYIGmOpIGh7FAzWwUQXg8J5c2AFbG6JaGsWeivWF6ujpmVAl8AjSsGIWmgpNmSZq9du3aPzJhzzrlInRxN91QzWynpEGCipMVpxk225WFpytPVKV9gNgwYBtCpU6edhjvnnNt1OdmCMbOV4XUN8ALQGVgddnsRXteE0UuA5rHqRcDKUF6UpLxcHUl1gAbAhmzMi3POueSqPcFI2l9S/UQ/0B34ABgP9A+j9QdeCv3jgb7hzLCWRAfzZ4bdaJskdQnHV/pVqJNoqzcwKRyncc45V01ysYvsUOCFcMy9DvC0mb0uaRYwTtIA4F9AHwAzWyBpHLAQKAUGmdn20NYNwEigHvBa6ACGA09KKibaculbHTPmnHNuh2pPMGa2FGiXpHw90C1FnXuBe5OUzwaOS1K+hZCgnHPO5UY+nabsnHOuBvEE45xzLis8wTjnnMsKTzDOOeeywhOMc865rPAE45xzLis8wTjnnMsKTzDOOeeywhOMc865rPAE45xzLis8wTjnnMsKTzDOOeeywhOMc865rPAE45xzLis8wTjnnMsKTzDOOeeyokYnGEk9JX0oqVjS4FzH45xztUmNTTCS9gb+BJwLHAtcIenY3EblnHO1R41NMEBnoNjMlprZ18AYoFeOY3LOuVpDZpbrGLJCUm+gp5l9P7z/HnCSmd0YG2cgMDC8PQb4cDcmeTCwbjfqV7dCireQYoXCireQYoXCireQYoVdj/cIM2uSbECd3YsnrylJWblsambDgGF7ZGLSbDPrtCfaqg6FFG8hxQqFFW8hxQqFFW8hxQrZibcm7yIrAZrH3hcBK3MUi3PO1To1OcHMAlpJainpP4C+wPgcx+Scc7VGjd1FZmalkm4E3gD2BkaY2YIsTnKP7GqrRoUUbyHFCoUVbyHFCoUVbyHFClmIt8Ye5HfOOZdbNXkXmXPOuRzyBOOccy4rPMHspkK4HY2k5ZLmS5oraXYoayRpoqQl4bVhDuMbIWmNpA9iZSnjk3RbWN4fSuqRB7H+QtKnYfnOlXRensTaXNLbkhZJWiDpplCer8s2Vbx5t3wl1ZU0U9K8EOsvQ3m+LttU8WZ32ZqZd7vYEZ088DHwLeA/gHnAsbmOK0mcy4GDK5T9Fhgc+gcD9+Uwvu8AHYAPKouP6LY/84B9gZZh+e+d41h/Afwsybi5jrUp0CH01wc+CjHl67JNFW/eLV+i6+wOCP37ADOALnm8bFPFm9Vl61swu6eQb0fTCxgV+kcBF+UqEDN7B9hQoThVfL2AMWa21cyWAcVEn0O1SBFrKrmOdZWZvRf6NwGLgGbk77JNFW8qOYvXIl+Gt/uEzsjfZZsq3lT2SLyeYHZPM2BF7H0J6X8QuWLABElzwu1xAA41s1UQ/bCBQ3IWXXKp4svXZX6jpPfDLrTEbpG8iVVSC+AEon+ueb9sK8QLebh8Je0taS6wBphoZnm9bFPEC1lctp5gdk+lt6PJE6eaWQeiO0sPkvSdXAe0G/JxmT8KHAm0B1YBD4TyvIhV0gHAc8DNZrYx3ahJyvIh3rxcvma23czaE90lpLOk49KMnvNlmyLerC5bTzC7pyBuR2NmK8PrGuAFok3d1ZKaAoTXNbmLMKlU8eXdMjez1eHH+w3wODt2JeQ8Vkn7EK2snzKz50Nx3i7bZPHm8/IN8X0OTAZ6ksfLNiEeb7aXrSeY3ZP3t6ORtL+k+ol+oDvwAVGc/cNo/YGXchNhSqniGw/0lbSvpJZAK2BmDuIrk1ihBBcTLV/IcaySBAwHFpnZg7FBeblsU8Wbj8tXUhNJB4X+esDZwGLyd9kmjTfry7a6zmKoqR1wHtHZLh8Dt+c6niTxfYvobJB5wIJEjEBj4C1gSXhtlMMYRxNtnm8j+uc0IF18wO1heX8InJsHsT4JzAfeDz/MpnkS62lEuzXeB+aG7rw8Xrap4s275Qu0Bf4ZYvoAuDOU5+uyTRVvVpet3yrGOedcVvguMuecc1nhCcY551xWeIJxzjmXFZ5gnHPOZYUnGOecc1nhCca5DEi6WJJJ+nauY6lI0peVj1Vu/K6SXs5WPM4leIJxLjNXAO8SXUybM5Jq7GPOXc3jCca5SoR7Y51KdFFl31h5V0mTJT0rabGkp8LV6Ej6jaSF4SaC94cbDS5V5CBJ3yTuCSfpH5KOCnddGCFplqR/SuoVhl8t6RlJfwcmpIkzXTw9Q9m7wCWxOqmm+YikO0N/D0nvSPL1hasS/zfkXOUuAl43s48kbZDUwcJt5Ynu+NuG6D5NU4FTJS0kuu3Gt83MJB1kZtslJZ5v0hKYA5wuaQZQZGbFkn4FTDKza8NtPWZKejNM52SgrZlV9qiAZPHMJrrP1FlEt10fGxv/9hTTHAzMkvQP4BHgPIvuV+VcxvwfiXOVu4LoWT+E1ytiw2aaWUlY+c4FWgAbgS3AXyRdAmwO4/6D6IFl3wF+TXRrlBOJ7mkH0X3iBodbqk8G6gKHh2ETM0guqeL5NrDMzJZYdOuOv8XGTzpNM9sMXAdMBP5oZh9nMG3nyvEtGOfSkNSY6J//cZKM6CmmJunnYZStsdG3A3XMrFRSZ6Ab0S61G0Mb/wB+ABwG3An8N9AVeCcxOeBSM/uwQgwnAV9lGPJO8YT+VPeESjrN4HhgfYjXuSrzLRjn0usNPGFmR5hZCzNrDiwj2vpIKhyzaWBmrwI3Ez1rA6KHZ50CfGNmW4i2MK4nSjwAbwA/ih03OWEPzcNioKWkI8P7+BZY0mlKOgK4hWiX27khyTlXJZ5gnEvvCqJn6MQ9B1yZpk594GVJ7wNTgJ8AmNlWoqcETg/j/SOMOz+8v5voUbbvS/ogvN9tIZkNBF4JB/k/iQ3eaZqx2+b/zKJnCQ0g2t1Xd0/E42oPv5uyc865rPAtGOecc1nhCcY551xWeIJxzjmXFZ5gnHPOZYUnGOecc1nhCcY551xWeIJxzjmXFf8fyJJ1mEHbBnEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Plot frequencies\n",
    "average_frequency = results_df_tuned['Average_Frequency'].mean()\n",
    "\n",
    "plt.bar(range(len(results_df_tuned['Average_Frequency'])), results_df_tuned['Average_Frequency'])\n",
    "plt.axhline(y=average_frequency, color='red', linestyle='--', label=f'Average ({average_frequency:.2f})')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Answer Index')\n",
    "plt.ylabel('Average Word Frequency')\n",
    "plt.title('Average Word Frequency per Answer')\n",
    "plt.legend()  # Add legend to explain the average line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776096bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38135c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do the f1 cars have some sort of air blowing motor attached to them on the starting grid? This is called a \"wind tunnel\" and it helps keep their tires from sticking too much when they\\'re first rolling off. I\\'ve seen pictures where you can see this in action, but how does that actually work?\\nI\\'m curious about all these different types of wind tunnels used for testing F1 cars. What are there? There\\'s no real-world racing going on here!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df['baseline_answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b2cf32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why do the f1 cars have some sort of air blowing motor attached to them on the starting grid? This is called a starter and it helps get their engines running. It's also why they don't start in circles, because if you're already revving your engine at 1000 RPM when someone else starts right next to you then there will be no point trying to pass each other as both are just going to stall before anyone can even make any progress.\\nI've always wondered what kind of technology would need to go into that system so I could replicate this without having to physically attach an external unit or something like that? Or maybe use one existing setup from another car which has been modified for racing purposes? The reason i'm curious about all this is since F1 drivers actually wear helmets with built-in sensors and computers that monitor everything around them while driving (including speed, acceleration, g-forces etc) but not much information seems available online regarding how these systems work internally.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df['fine_tuned_answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "835418df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Why is Coca Cola cheaper in some places than clean water, e.g. Africa, Tanzania, Zambia, etc?\n",
      "Model Output: Why is Coca Cola cheaper in some places than clean water, e.g. Africa, Tanzania, Zambia, etc? The answer lies with the local currency and how it's used to purchase things like food or other products that are sold at a lower cost when bought locally.\n",
      "The prices of goods vary depending on where you live because they're influenced by several factors: supply/demand for different types of product; taxes (especially sales tax); competition from local businesses; advertising costs; marketing efforts; and so forth. These various factors can cause prices to fluctuate over time as well as across locations within an area.\n",
      "Base Model Output: Why is Coca Cola cheaper in some places than clean water, e.g. Africa, Tanzania, Zambia, etc? In the US it's not that bad but if you look at a few countries like India or China then prices are pretty similar to soda. I think this has more to do with taxes and regulations on how much can be charged for certain products (soda) vs what other costs there might be such as electricity cost per liter of water. So just because someone could charge $1 less doesn't mean they have enough money left over to cover their expenses from selling it.\n",
      "Also why does Coca Cola make so little profit compared to Pepsi? They're both owned by different companies which own them separately. This means people buy either one brand depending on where they live and whether they want Coke or Pepsi. People who don't drink coke will still get cola when buying Pepsi, even though those two brands aren't directly related.\n",
      "--------------------------------------------------\n",
      "Question 2: Is there anything between galaxies?\n",
      "Model Output: Is there anything between galaxies??\n",
      "> I'm reading a book that claims the universe is infinite in size, and it's made up of an endless number of galaxies. That sounds like science fiction.\n",
      "The first part you've got right: The observable Universe (the portion we can see) is finite in space but has no edges or boundaries. But this doesn't mean everything else isn't out there - even if we cannot observe them directly, they are all around us. There are also regions within our own galaxy where new stars have been born recently enough to be seen by telescopes. This includes nebulae which emit light from newly formed hydrogen gas clouds being ionized as more star formation takes place nearby.  > How does gravity work on these vast distances so well hold things together? Gravity is not \"well\" at those enormous scales because its strength decreases with distance. You could imagine looking down into one of those distant galaxies and seeing nothing moving past your eyesight line except for what was emitted before the last time the\n",
      "Base Model Output: Is there anything between galaxies? And if so, what is the nature of that thing?\n",
      "> I've heard some people say \"there are no objects between us and other galaxies\" but how do you define an object in this context? Is it a black hole or something else entirely?\n",
      "\n",
      "The short answer to your question is: we don't know. The definition varies depending on which part of cosmology you're talking about (general relativity vs. quantum gravity). If we assume general relativity then any massless particle can be considered as having zero size - however, even these particles have finite spatial extent due to interactions with matter and radiation. In contrast, under quantum gravity terms like strings would not behave like ordinary classical objects. So for example they'd look very different from photons, which also interact via gravitational fields.\n",
      "\n",
      "In fact our current understanding suggests that most structures within galaxy clusters consist mostly of dark energy filled voids rather than normal intergalactic gas clouds. But again, these things aren't really well defined either!\n",
      "--------------------------------------------------\n",
      "Question 3: Can someone explain Newtons three laws?\n",
      "Model Output: Can someone explain Newtons three laws? The first two are pretty well known. I'm having trouble understanding the third one, which is: \"If you apply a force to an object at rest and let it go free (i.e., not accelerate) then that same amount of force will be applied in all directions equally.\"\n",
      "This seems counterintuitive because if you're moving towards something with no net acceleration, how can there still be equal forces acting on both sides of your body from every direction?\n",
      "\n",
      "I've read some explanations online but none seem quite right for me so hopefully this question gets answered here!\n",
      "Base Model Output: Can someone explain Newtons three laws? The first two are pretty obvious, but the third is a bit more tricky. It's about how to calculate acceleration when you know velocity and position (or distance traveled). I think it has something to do with finding an average rate of change in direction over time... or something like that.\n",
      "\n",
      "If anyone can help me out here, please let me know! Thanks!\n",
      "\n",
      "I'm trying to figure this one out on my own for now, but I'd appreciate any clarification if possible.\n",
      "--------------------------------------------------\n",
      "Question 4: Did ancient people have broader noses and facial structure, or is that just an artistic liberty?\n",
      "Model Output: Did ancient people have broader noses and facial structure, or is that just an artistic liberty? The answer lies in the history of art. In classical Greece, nose shapes varied from a straight to curved shape (often depicted as being more rounded). There's also evidence suggesting that they had larger eyes than we do today; for example, the Greek historian Herodotus noted that \"the inhabitants of Egypt are not like other nations\" because their large round faces were very striking against the pale complexion typical of Greeks.  & #x200B;\n",
      "\n",
      "I hope this answers your question! Let me know if you'd like any further clarification on anything.\n",
      "Base Model Output: Did ancient people have broader noses and facial structure, or is that just an artistic liberty? The concept of \"nose\" as we know it today isn't really a thing until the 19th century. So how did our ancestors' faces look like before then?\n",
      "To answer your question, I'd suggest looking into some anthropology studies on prehistoric humans. They might give you more insight than any other source!\n",
      "--------------------------------------------------\n",
      "Question 5: Can we quantify the size of the internet?\n",
      "Model Output: Can we quantify the size of the internet? It's hard to think about it because there are so many different types of data and networks, but if you break down into smaller components like people and devices connected through various mediums (e.g. fiber optic cables), I suppose that could be done in some way.\n",
      "It might not even make sense from a practical standpoint though - what would you do with this information? We're talking millions upon millions of interconnected computers all over the world...how exactly does one compare their \"size\" or how much they contribute to something as large as an entire country's economy?\n",
      "I guess my question is more philosophical than technical. Is quantifying the Internet really necessary for any reason other than maybe understanding its impact on society at larger scales?\n",
      "Base Model Output: Can we quantify the size of the internet? It's a very large scale system, and it can be hard to measure. How do you define what constitutes \"the\" Internet?\n",
      "\n",
      "The term is used in different contexts with varying definitions: e.g., the physical network infrastructure (e.g., cables) or the data content itself; this latter definition refers to something like Google search results. In general though, when referring to the entire web as an entity, one might say that its total number of pages has been measured at around 1 trillion, but for other types of networks, such as fiber optic cables which are much smaller than those on the surface level, there may not even exist any reliable way to count them.\n",
      "\n",
      "In short, measuring how big the whole thing is is difficult because many things aren't easily quantifiable, especially if they don't have clear boundaries - think about all the bits between your house and the nearest cell tower.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Question {i+1}: {sampled_data[i]}\")\n",
    "    print(f\"Model Output: {all_generated_texts[i]}\")\n",
    "    print(f\"Base Model Output: {all_base_generated_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a612fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge-score in ./.local/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge-score) (3.6.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in ./.local/lib/python3.9/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from rouge-score) (1.20.3)\n",
      "Requirement already satisfied: click in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge-score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk->rouge-score) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e0aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Function to evaluate a pair of answers\n",
    "def evaluate_concreteness(baseline_answers, fine_tuned_answers):\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = [], [], [], []\n",
    "\n",
    "    for base_answer, fine_answer in zip(baseline_answers, fine_tuned_answers):\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = scorer.score(base_answer, fine_answer)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([base_answer.split()], fine_answer.split())\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    return rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores\n",
    "\n",
    "# Batch evaluation function\n",
    "def evaluate_in_batches(all_base_generated_texts, all_fine_generated_texts, batch_size=10):\n",
    "    rouge1_all, rouge2_all, rougeL_all, bleu_all = [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_base_generated_texts), batch_size)):\n",
    "        # Get the current batch\n",
    "        base_batch = all_base_generated_texts[i:i + batch_size]\n",
    "        fine_batch = all_fine_generated_texts[i:i + batch_size]\n",
    "\n",
    "        # Evaluate the batch\n",
    "        rouge1, rouge2, rougeL, bleu = evaluate_concreteness(base_batch, fine_batch)\n",
    "\n",
    "        # Aggregate the scores\n",
    "        rouge1_all.extend(rouge1)\n",
    "        rouge2_all.extend(rouge2)\n",
    "        rougeL_all.extend(rougeL)\n",
    "        bleu_all.extend(bleu)\n",
    "\n",
    "    return rouge1_all, rouge2_all, rougeL_all, bleu_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4b91aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:01<00:01, 10.38it/s]/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:13<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.4491\n",
      "Average ROUGE-2 F1: 0.1890\n",
      "Average ROUGE-L F1: 0.2713\n",
      "Average BLEU Score: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = evaluate_in_batches(\n",
    "    output_df['fine_tuned_answer'], output_df['baseline_answer'], batch_size=10\n",
    ")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "073a9bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:08<00:00, 39.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:08<00:00, 39.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Perplexity: 40.71963119506836\n",
      "Fine-tuned Model Perplexity: 40.71963119506836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, device):\n",
    "    model.eval()\n",
    "    total_log_likelihood = 0\n",
    "    total_words = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts):\n",
    "            # Tokenize the text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "            # Get the model's output (logits)\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift input_ids and logits for next-token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "            # Calculate log probabilities\n",
    "            log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "            shift_labels = shift_labels.unsqueeze(-1)\n",
    "            selected_log_probs = torch.gather(log_probs, -1, shift_labels).squeeze(-1)\n",
    "\n",
    "            # Sum the log-likelihood of the predicted tokens\n",
    "            total_log_likelihood += selected_log_probs.sum().item()\n",
    "            total_words += shift_labels.numel()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    avg_log_likelihood = total_log_likelihood / total_words\n",
    "    perplexity = torch.exp(torch.tensor(-avg_log_likelihood))\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "baseline_perplexity = calculate_perplexity(base_model, AutoTokenizer.from_pretrained(peft_model_id), sampled_data, device)\n",
    "fine_tuned_perplexity = calculate_perplexity(model, AutoTokenizer.from_pretrained(peft_model_id), sampled_data, device)\n",
    "\n",
    "print(f\"Baseline Model Perplexity: {baseline_perplexity}\")\n",
    "print(f\"Fine-tuned Model Perplexity: {fine_tuned_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69050922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above is fine tune Llama with original eli5 with 325K rows in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507fc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "df_val=dataset['validation'].to_pandas()\n",
    "df_test=dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f08f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(pd.concat([df_train['question'], df_val['question'],df_test['question']], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ef6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = vectorizer.transform(df_train['question'])\n",
    "val_tfidf = vectorizer.transform(df_val['question'])\n",
    "test_tfidf = vectorizer.transform(df_test['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601fe013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 1000\n",
    "similar_pairs = []\n",
    "\n",
    "# Iterate through the validation set in batches\n",
    "for start_idx in range(0, val_tfidf.shape[0], batch_size):\n",
    "    end_idx = min(start_idx + batch_size, val_tfidf.shape[0])\n",
    "    val_batch = val_tfidf[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity for the current batch\n",
    "    sim_matrix = cosine_similarity(train_tfidf, val_batch)\n",
    "\n",
    "    # Filter pairs with similarity above the threshold (e.g., 0.8)\n",
    "    threshold = 0.6\n",
    "    train_idx, val_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Store the results\n",
    "    for i in range(len(train_idx)):\n",
    "        similar_pairs.append((\n",
    "            train_idx[i],\n",
    "            start_idx + val_idx[i],\n",
    "            sim_matrix[train_idx[i], val_idx[i]]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a04bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_similar_pairs = sorted(similar_pairs, key=lambda x: x[2], reverse=True)[-10:]\n",
    "len(similar_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36bd95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Pair:\n",
      "Question 1: The Universe is expanding, but what is it expanding into?\n",
      "Question 2: If the universe is expanding, what is it expanding into? What exists in the space that the universe is expanding into?\n",
      "Similarity Score: 0.9010\n",
      "\n",
      "Top 2 Pair:\n",
      "Question 1: Why do we cry when we're upset?\n",
      "Question 2: why do we cry when we get upset?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 3 Pair:\n",
      "Question 1: Where do asteroids in space come from?\n",
      "Question 2: Where do asteroids come from?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 4 Pair:\n",
      "Question 1: Can the sound barrier be broken underwater?\n",
      "Question 2: Have we broken the sound barrier underwater?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 5 Pair:\n",
      "Question 1: If every action has an equal and opposite reaction, what is the opposite reaction of the big bang.\n",
      "Question 2: If every action has an equal and opposite reaction, what's the opposite reaction to gravity?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 6 Pair:\n",
      "Question 1: Why is \"good morning\" two words, but \"goodnight\" is only one?\n",
      "Question 2: why is, \"goodnight,\" one word, but, \"good morning,\" two words?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 7 Pair:\n",
      "Question 1: What is the difference between a nuclear fission and a fusion bomb?\n",
      "Question 2: What is the difference between nuclear fission and fusion?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 8 Pair:\n",
      "Question 1: What is the difference between a nuclear fission and a fusion bomb?\n",
      "Question 2: What the difference is between nuclear fusion and fission?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 9 Pair:\n",
      "Question 1: Why doesnâ€™t cookie dough ice cream have a warning to â€œnot eat raw cookie doughâ€?\n",
      "Question 2: Why is it bad to eat raw cookie dough when there is cookie dough ice cream?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 10 Pair:\n",
      "Question 1: What is a trust fund?\n",
      "Question 2: What a \"Trust Fund Baby\" is and exactly how a \"trust fund\" works.\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 11 Pair:\n",
      "Question 1: 'long exposure' in photography\n",
      "Question 2: How does long exposure photography work?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 12 Pair:\n",
      "Question 1: What is the difference between a \"theory\" and a \"law\" (scientific terms)\n",
      "Question 2: What is the difference between a scientific theory and law?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 13 Pair:\n",
      "Question 1: What exactly is existentialism?\n",
      "Question 2: What is Existentialism?\n",
      "Similarity Score: 0.9002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_pairs = [pair for pair in similar_pairs if 0.9 <= pair[2] <= 0.901]\n",
    "\n",
    "# Sort the filtered pairs if needed\n",
    "filtered_pairs = sorted(filtered_pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Display the results\n",
    "\n",
    "for i, (idx1, idx2, score) in enumerate(filtered_pairs,start=0):\n",
    "    question1 = df_train['question'][idx1]\n",
    "    question2 = df_val['question'][idx2]\n",
    "    print(f\"Top {i+1} Pair:\")\n",
    "    print(f\"Question 1: {question1}\")\n",
    "    print(f\"Question 2: {question2}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b37ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Pair:\n",
      "Question 1: How can scissors be left or right handed?\n",
      "Question 2: Why are more people Right handed than left?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 2 Pair:\n",
      "Question 1: If we ever can achieve faster than light travel, would there only be a sonic boom after breaking the sound barrier? What would happen once we break the light barrier?\n",
      "Question 2: Why does breaking the sound barrier create a sonic boom?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 3 Pair:\n",
      "Question 1: Who exactly Che Guevara was and how he became a counterculture icon.\n",
      "Question 2: Why is Che Guevara such a pop culture icon?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 4 Pair:\n",
      "Question 1: Why is voting so screwed up in Florida?\n",
      "Question 2: Why is Africa so screwed up?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 5 Pair:\n",
      "Question 1: Why when you turn a glass of liquid, the glass moves but the liquid stays in the same position\n",
      "Question 2: Is glass really a liquid?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 6 Pair:\n",
      "Question 1: if you know what a placebo is, is it less likely the placebo effect could happen to you?\n",
      "Question 2: What is necessary for the placebo effect?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 7 Pair:\n",
      "Question 1: Why do bridges become icy/freeze before the regular roads do?\n",
      "Question 2: Why do bridges ice before roads?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 8 Pair:\n",
      "Question 1: Is it possible to know the charge of a black hole?\n",
      "Question 2: What is a Black Hole?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 9 Pair:\n",
      "Question 1: (x-post /askreddit ) How long would it take an obese person to starve to death?\n",
      "Question 2: How long would it take the fattest person to starve to death?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 10 Pair:\n",
      "Question 1: What would happen if the moon was destroyed?\n",
      "Question 2: What would happen if the moon would disappear?\n",
      "Similarity Score: 0.6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the top 5 most similar question pairs\n",
    "for i, (idx1, idx2, score) in enumerate(top_similar_pairs):\n",
    "    question1 = df_train['question'][idx1]\n",
    "    question2 = df_val['question'][idx2]\n",
    "    print(f\"Top {i+1} Pair:\")\n",
    "    print(f\"Question 1: {question1}\")\n",
    "    print(f\"Question 2: {question2}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b02043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract similarity scores\n",
    "similarity_scores = [pair[2] for pair in similar_pairs]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(similarity_scores, bins=50, color='skyblue')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores >0.6 between Train and Valiadation Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9e3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 1000\n",
    "similar_pairs = []\n",
    "\n",
    "# Iterate through the validation set in batches\n",
    "for start_idx in range(0, test_tfidf.shape[0], batch_size):\n",
    "    end_idx = min(start_idx + batch_size, test_tfidf.shape[0])\n",
    "    test_batch = test_tfidf[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity for the current batch\n",
    "    sim_matrix = cosine_similarity(train_tfidf, test_batch)\n",
    "\n",
    "    # Filter pairs with similarity above the threshold (e.g., 0.8)\n",
    "    threshold = 0.6\n",
    "    train_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Store the results\n",
    "    for i in range(len(train_idx)):\n",
    "        similar_pairs.append((\n",
    "            train_idx[i],\n",
    "            start_idx + test_idx[i],\n",
    "            sim_matrix[train_idx[i], test_idx[i]]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f1ea748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract similarity scores\n",
    "similarity_scores = [pair[2] for pair in similar_pairs]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(similarity_scores, bins=50, color='skyblue')\n",
    "plt.xlabel('Cosine Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores >0.6 between Train and Valiadation Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21475417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Pair:\n",
      "Question 1: What really are radio waves and do they travel at the speed of light?\n",
      "Question 2: Speed of light travel?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 2 Pair:\n",
      "Question 1: Why do dogs tilt their head sometimes when they hear certain sounds?\n",
      "Question 2: why do dogs tilt their head when you talk to them?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 3 Pair:\n",
      "Question 1: Why does irish music sound like pirate music?\n",
      "Question 2: What does music sound like to animals?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 4 Pair:\n",
      "Question 1: If the speed of light is a universal constant (~300,000 kms/s) then how did the universe expand so quickly after the big bang?\" Does this imply that \"out there\" there is \"something\" faster than the speed of light?\n",
      "Question 2: if the speed of light is the universal speed limit. After the big bang how did the universe expand at speeds wayyy faster than the speed of light?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 5 Pair:\n",
      "Question 1: Why do we have different blood types?\n",
      "Question 2: Why are there different blood types? Have they always been around? Could a new blood type surface?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 6 Pair:\n",
      "Question 1: Why do we have different blood types?\n",
      "Question 2: Why are there different blood types? Have they always been around? Could a new blood type surface?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 7 Pair:\n",
      "Question 1: Why Do We Have Different Blood Types?\n",
      "Question 2: Why are there different blood types? Have they always been around? Could a new blood type surface?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 8 Pair:\n",
      "Question 1: why do we have different blood-types?\n",
      "Question 2: Why are there different blood types? Have they always been around? Could a new blood type surface?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 9 Pair:\n",
      "Question 1: The difference between an Empire, Kingdom, Duchy and Republic.\n",
      "Question 2: Whats the difference between a Kingdom and an Empire?\n",
      "Similarity Score: 0.6000\n",
      "\n",
      "Top 10 Pair:\n",
      "Question 1: What is beyond the Cosmic Microwave Background?\n",
      "Question 2: Cosmic Microwave Background (CMB) and age of the universe\n",
      "Similarity Score: 0.6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_similar_pairs = sorted(similar_pairs, key=lambda x: x[2], reverse=True)[-10:]\n",
    "\n",
    "# Display the top 5 most similar question pairs\n",
    "for i, (idx1, idx2, score) in enumerate(top_similar_pairs):\n",
    "    question1 = df_train['question'][idx1]\n",
    "    question2 = df_test['question'][idx2]\n",
    "    print(f\"Top {i+1} Pair:\")\n",
    "    print(f\"Question 1: {question1}\")\n",
    "    print(f\"Question 2: {question2}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc299fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "threshold = 0.8\n",
    "batch_size = 1000\n",
    "\n",
    "# Lists to store indices to remove\n",
    "train_remove_indices = set()\n",
    "validation_remove_indices = set()\n",
    "test_remove_indices = set()\n",
    "\n",
    "# Compute similarity between train and validation sets, and store indices for removal\n",
    "for start_idx in range(0, val_tfidf.shape[0], batch_size):\n",
    "    end_idx = min(start_idx + batch_size, val_tfidf.shape[0])\n",
    "    val_batch = val_tfidf[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between train and validation batches\n",
    "    sim_matrix = cosine_similarity(train_tfidf, val_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    train_idx, val_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both train and validation\n",
    "    for i in range(len(train_idx)):\n",
    "        train_remove_indices.add(train_idx[i])\n",
    "        validation_remove_indices.add(start_idx + val_idx[i])\n",
    "\n",
    "# Compute similarity between train and test sets, and store indices for removal\n",
    "for start_idx in range(0, test_tfidf.shape[0], batch_size):\n",
    "    end_idx = min(start_idx + batch_size, test_tfidf.shape[0])\n",
    "    test_batch = test_tfidf[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between train and test batches\n",
    "    sim_matrix = cosine_similarity(train_tfidf, test_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    train_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both train and test\n",
    "    for i in range(len(train_idx)):\n",
    "        train_remove_indices.add(train_idx[i])\n",
    "        test_remove_indices.add(start_idx + test_idx[i])\n",
    "\n",
    "# Compute similarity between validation and test sets, and store indices for removal\n",
    "for start_idx in range(0, test_tfidf.shape[0], batch_size):\n",
    "    end_idx = min(start_idx + batch_size, test_tfidf.shape[0])\n",
    "    test_batch = test_tfidf[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between validation and test batches\n",
    "    sim_matrix = cosine_similarity(val_tfidf, test_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    val_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both validation and test\n",
    "    for i in range(len(val_idx)):\n",
    "        validation_remove_indices.add(val_idx[i])\n",
    "        test_remove_indices.add(start_idx + test_idx[i])\n",
    "\n",
    "# Convert to lists and sort\n",
    "train_remove_indices = sorted(train_remove_indices)\n",
    "validation_remove_indices = sorted(validation_remove_indices)\n",
    "test_remove_indices = sorted(test_remove_indices)\n",
    "\n",
    "# Filter out the similar pairs from the DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508d80a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train set size: 256905\n",
      "Filtered validation set size: 27430\n",
      "Filtered test set size: 30435\n"
     ]
    }
   ],
   "source": [
    "filtered_train_df = df_train.drop(index=train_remove_indices).reset_index(drop=True)\n",
    "filtered_val_df = df_val.drop(index=validation_remove_indices).reset_index(drop=True)\n",
    "filtered_test_df = df_test.drop(index=test_remove_indices).reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered train set size: {len(filtered_train_df)}\")\n",
    "print(f\"Filtered validation set size: {len(filtered_val_df)}\")\n",
    "print(f\"Filtered test set size: {len(filtered_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6192b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_df.to_csv('filtered_train_df.csv', index=False)\n",
    "filtered_val_df.to_csv('filtered_val_df.csv', index=False)\n",
    "filtered_test_df.to_csv('filtered_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad814534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above used TFIDF to remove train,validation and test dataset simliarity higher than 0.8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fafbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filtered_train_df=pd.read_csv('filtered_train_df.csv')\n",
    "filtered_val_df=pd.read_csv('filtered_val_df.csv')\n",
    "filtered_test_df=pd.read_csv('filtered_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee972e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.25.2)\n",
      "Requirement already satisfied: scikit-learn in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scipy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: sympy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.9)\n",
      "Requirement already satisfied: triton==3.0.0 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.20.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21b4243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinmei/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 1000  # Adjust based on available memory\n",
    "threshold = 0.8 \n",
    "\n",
    "train_remove_indices = set()\n",
    "validation_remove_indices = set()\n",
    "test_remove_indices = set()\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define embedding_dim based on a sample sentence\n",
    "sample_embedding = model.encode([\"This is a short sentence.\"])\n",
    "embedding_dim = len(sample_embedding[0])\n",
    "\n",
    "# Initialize memory-mapped arrays for train, validation, and test embeddings\n",
    "train_embeddings = np.memmap('train_embeddings.npy', dtype='float32', mode='w+', shape=(len(filtered_train_df), embedding_dim))\n",
    "val_embeddings = np.memmap('val_embeddings.npy', dtype='float32', mode='w+', shape=(len(filtered_val_df), embedding_dim))\n",
    "test_embeddings = np.memmap('test_embeddings.npy', dtype='float32', mode='w+', shape=(len(filtered_test_df), embedding_dim))\n",
    "\n",
    "# Encode and save train embeddings in batches\n",
    "for i in range(0, len(filtered_train_df['question']), batch_size):\n",
    "    batch = filtered_train_df.iloc[i:i + batch_size]['question']\n",
    "    batch_embeddings = model.encode(batch.tolist())\n",
    "    train_embeddings[i:i + batch_size] = batch_embeddings  # Write directly to memmap\n",
    "\n",
    "# Encode and save validation embeddings in batches\n",
    "for i in range(0, len(filtered_val_df['question']), batch_size):\n",
    "    batch = filtered_val_df.iloc[i:i + batch_size]['question']\n",
    "    batch_embeddings = model.encode(batch.tolist())\n",
    "    val_embeddings[i:i + batch_size] = batch_embeddings  # Write directly to memmap\n",
    "\n",
    "# Encode and save test embeddings in batches\n",
    "for i in range(0, len(filtered_test_df['question']), batch_size):\n",
    "    batch = filtered_test_df.iloc[i:i + batch_size]['question']\n",
    "    batch_embeddings = model.encode(batch.tolist())\n",
    "    test_embeddings[i:i + batch_size] = batch_embeddings  # Write directly to memmap\n",
    "\n",
    "# Compute similarity between train and validation sets in batches and store indices for removal\n",
    "for start_idx in range(0, len(filtered_val_df), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(filtered_val_df))\n",
    "    val_batch = val_embeddings[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between train and validation batches\n",
    "    sim_matrix = cosine_similarity(train_embeddings, val_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    train_idx, val_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both train and validation\n",
    "    for i in range(len(train_idx)):\n",
    "        train_remove_indices.add(train_idx[i])\n",
    "        validation_remove_indices.add(start_idx + val_idx[i])\n",
    "\n",
    "# Compute similarity between train and test sets in batches and store indices for removal\n",
    "for start_idx in range(0, len(filtered_test_df), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(filtered_test_df))\n",
    "    test_batch = test_embeddings[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between train and test batches\n",
    "    sim_matrix = cosine_similarity(train_embeddings, test_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    train_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both train and test\n",
    "    for i in range(len(train_idx)):\n",
    "        train_remove_indices.add(train_idx[i])\n",
    "        test_remove_indices.add(start_idx + test_idx[i])\n",
    "        \n",
    "\n",
    "\n",
    "# Compute similarity between validation and test sets in batches and store indices for removal\n",
    "for start_idx in range(0, len(filtered_test_df), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(filtered_test_df))\n",
    "    test_batch = test_embeddings[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between validation and test batches\n",
    "    sim_matrix = cosine_similarity(val_embeddings, test_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    val_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "\n",
    "    # Collect indices for removal from both validation and test\n",
    "    for i in range(len(val_idx)):\n",
    "        validation_remove_indices.add(val_idx[i])\n",
    "        test_remove_indices.add(start_idx + test_idx[i])\n",
    "\n",
    "# Convert to lists and sort\n",
    "train_remove_indices = sorted(train_remove_indices)\n",
    "validation_remove_indices = sorted(validation_remove_indices)\n",
    "test_remove_indices = sorted(test_remove_indices)\n",
    "\n",
    "# You can now drop the rows from the original dataframes using these indices\n",
    "filter_train_df = filtered_train_df.drop(index=train_remove_indices).reset_index(drop=True)\n",
    "filter_validation_df = filtered_val_df.drop(index=validation_remove_indices).reset_index(drop=True)\n",
    "filter_test_df = filtered_test_df.drop(index=test_remove_indices).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbff7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_train_df.to_csv('filter_train_df.csv', index=False)\n",
    "filter_validation_df.to_csv('filter_validation_df.csv', index=False)\n",
    "filter_test_df.to_csv('filter_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0643e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs=[]\n",
    "for start_idx in range(0, len(filtered_test_df), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(filtered_test_df))\n",
    "    test_batch = test_embeddings[start_idx:end_idx]\n",
    "\n",
    "    # Compute cosine similarity between train and test batches\n",
    "    sim_matrix = cosine_similarity(train_embeddings, test_batch)\n",
    "\n",
    "    # Get indices where similarity is above the threshold\n",
    "    train_idx, test_idx = np.where(sim_matrix > threshold)\n",
    "    for i in range(len(train_idx)):\n",
    "        similar_pairs.append((\n",
    "            train_idx[i],\n",
    "            start_idx + test_idx[i],\n",
    "            sim_matrix[train_idx[i], test_idx[i]]\n",
    "        ))\n",
    "top_similar_pairs = sorted(similar_pairs, key=lambda x: x[2], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0409db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs = [pair for pair in similar_pairs if 0.9 <= pair[2] <= 0.901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498c8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Pair:\n",
      "Question 1: How come warm water tastes so bad?\n",
      "Question 2: Why does cold water taste great, and warm water taste terrible?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 2 Pair:\n",
      "Question 1: How do deep sea creatures withstand the pressure?\n",
      "Question 2: How does the cellular structure of deep sea creatures allow them to withstand such immense pressure?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 3 Pair:\n",
      "Question 1: What sort of relationship is there between light and time?\n",
      "Question 2: Is time and light unrelated?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 4 Pair:\n",
      "Question 1: How come our non-dominant hand is less capable than our dominant hand? Why isnâ€™t everyone ambidextrous?\n",
      "Question 2: Why do people have a dominant hand? Wouldn't it be more advantageous to just be ambidextrous?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 5 Pair:\n",
      "Question 1: the difference in sorting comments between \"best\" and \"top\"\n",
      "Question 2: Whats the difference between \"Top\" and \"Best\" when filtering the comments?\n",
      "Similarity Score: 0.9010\n",
      "\n",
      "Top 6 Pair:\n",
      "Question 1: If all our cells get replaced every 7 years, what is keeping tattoo's so permanent?\n",
      "Question 2: If every cell in the body regenerates over the course of ~10 years, how are tattoos permanent over decades?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 7 Pair:\n",
      "Question 1: . Why does warm water clean dishes so much better than cold water?\n",
      "Question 2: why does hot water work better for washing dishes or just dirt in general than cold water?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 8 Pair:\n",
      "Question 1: Why does my string cheese taste better when I eat it in strings, as opposed to just biting big chunks off?\n",
      "Question 2: Why Does String Cheese taste better when pulled apart, rather than when just biting into it?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 9 Pair:\n",
      "Question 1: What's the difference between hard water & soft water?\n",
      "Question 2: What is the difference between hard and soft water and why do different areas have different hardness?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 10 Pair:\n",
      "Question 1: If the camera lenses are round, How is the picture we see square?\n",
      "Question 2: why pictures are square but camere lenses are round?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 11 Pair:\n",
      "Question 1: Do most galaxies have a single black hole at the center?\n",
      "Question 2: Do all galaxies have a black hole at their center? Are there any black holes that do not have galaxies around them?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 12 Pair:\n",
      "Question 1: Why does the pitch of voices go up or down when you speed up or slow down a video?\n",
      "Question 2: Why do voices and sounds in video go deeper/lower in pitch when the video is slowed down?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 13 Pair:\n",
      "Question 1: Why do certain foods taste so different -- and often so much better or worse -- at different temperatures?\n",
      "Question 2: Why do certain foods and drinks taste different at different temperatures?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 14 Pair:\n",
      "Question 1: How do clouds float if they are so heavy?\n",
      "Question 2: Why do clouds float when they have tons of water in them?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 15 Pair:\n",
      "Question 1: Why do most people have such negative feelings towards tickling?\n",
      "Question 2: Why do some people hate tickling while others love it?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 16 Pair:\n",
      "Question 1: How do Enchroma Glasses allow people to see colour?\n",
      "Question 2: How do Enchroma glasses work to allow the wearer to see colors again?\n",
      "Similarity Score: 0.9010\n",
      "\n",
      "Top 17 Pair:\n",
      "Question 1: What is the difference between an atheist, an agnostic and a secular?\n",
      "Question 2: Difference between agnostics and atheists.\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 18 Pair:\n",
      "Question 1: How are different cheeses made? (i.e swiss, cheddar, blue cheese, etc)\n",
      "Question 2: How are different cheeses made?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 19 Pair:\n",
      "Question 1: What are tonsils, what do they do and why is okay for us to have them removed?\n",
      "Question 2: What is the purpose of tonsils? Many get them removed and the only time I hear of them is when people get tonsillitis\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 20 Pair:\n",
      "Question 1: What is hydrogen peroxide doing when used on a cut/wound?\n",
      "Question 2: How does hydrogen peroxide clean wounds?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 21 Pair:\n",
      "Question 1: How much have skyscrapers slowed down Earth's rotation speed?\n",
      "Question 2: Do skyscrapers slow down the earths rotation?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 22 Pair:\n",
      "Question 1: Whats the difference between the Navy, the Coast Guard, and the Marines?\n",
      "Question 2: What is the difference between the navy and the marines?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 23 Pair:\n",
      "Question 1: How does the extra air in potato chip bags actually preserve the chips?\n",
      "Question 2: Is there anything special about the air in potato chip bags?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 24 Pair:\n",
      "Question 1: Why is there a debt ceiling at all when all they do is continually raise it?\n",
      "Question 2: Can someone explain why the US has a debt ceiling if they just keep raising it?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 25 Pair:\n",
      "Question 1: What exactly is happening in Ferguson?\n",
      "Question 2: What's the deal in Ferguson right now?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 26 Pair:\n",
      "Question 1: Why does body hair stop growing after a certain length, yet head hair doesn't?\n",
      "Question 2: Why does the hair on our arms (or other various parts) stop growing at a certain length but the hair on our head continues to grow?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 27 Pair:\n",
      "Question 1: What is an economic \"bubble\"? What causes it to \"pop\"?\n",
      "Question 2: What is an economic bubble?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 28 Pair:\n",
      "Question 1: How is Reddit link and comment karma calculated?\n",
      "Question 2: How does Reddit Karma calculate?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 29 Pair:\n",
      "Question 1: Why can strobing/flickering lights cause epileptic seizures?\n",
      "Question 2: Why do epileptics get seizures because of strobe lights?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 30 Pair:\n",
      "Question 1: why do people hate Obamacare?\n",
      "Question 2: Why do Republicans hate Obamacare so much?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 31 Pair:\n",
      "Question 1: Why do people hate Obamacare?\n",
      "Question 2: Why do Republicans hate Obamacare so much?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 32 Pair:\n",
      "Question 1: Why do so many people hate Obamacare?\n",
      "Question 2: Why do Republicans hate Obamacare so much?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 33 Pair:\n",
      "Question 1: Is there any hard evidence that GMO foods are harmful?\n",
      "Question 2: Many foods these days boast not containing any genetically modified organisms (GMO's). Is there any scientific evidence that GMO's are harmful to human health?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 34 Pair:\n",
      "Question 1: What do producers and executive producers do?\n",
      "Question 2: What do producers and production companies actually do?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 35 Pair:\n",
      "Question 1: What would happen if a male started taking birth control?\n",
      "Question 2: What happens if a male takes birth control pills?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 36 Pair:\n",
      "Question 1: Friday Free-for-All -- October 25, 2013\n",
      "Question 2: Friday Free-for-All | June 08, 2018\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 37 Pair:\n",
      "Question 1: how do they convert old movies into HD?\n",
      "Question 2: How are really old movies and TV shows up-converted to HD?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 38 Pair:\n",
      "Question 1: How does the human body measure temperature?\n",
      "Question 2: What is being measured when we measure our body temperature?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 39 Pair:\n",
      "Question 1: Why (seemingly) so many Russians have dashboard cameras\n",
      "Question 2: Why do so many russians have dash cameras?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 40 Pair:\n",
      "Question 1: How do linguists know the pronounciation of syllables/letters in dead languages?\n",
      "Question 2: How do linguists know how words from dead languages are pronounced?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 41 Pair:\n",
      "Question 1: Why is light affected by gravity if the equation for gravity requires mass?\n",
      "Question 2: How does gravity affect light if light has no mass?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 42 Pair:\n",
      "Question 1: Why do mosquito bite itch?\n",
      "Question 2: Why does a mosquito bite itch more if you kill it in the act?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 43 Pair:\n",
      "Question 1: Why do most animals know how to swim instinctively, but Humans do not?\n",
      "Question 2: Why do animals instinctively know how to swim whereas humans generally just drown without the proper training?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 44 Pair:\n",
      "Question 1: What exactly is radiation and why is it associated with DNA mutations?\n",
      "Question 2: What is it radiation does that causes mutations?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 45 Pair:\n",
      "Question 1: Why can't the world agree on one universal language?\n",
      "Question 2: Why doesn't the world use a universal language?\n",
      "Similarity Score: 0.9010\n",
      "\n",
      "Top 46 Pair:\n",
      "Question 1: How does radiation cause mutations in animals or humans?\n",
      "Question 2: What is it radiation does that causes mutations?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 47 Pair:\n",
      "Question 1: If people with one eye have no depth perception, how come I still have it with one eye closed?\n",
      "Question 2: Why is it that if I close one eye, I still perceive depth?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 48 Pair:\n",
      "Question 1: Why does our recorded voice sound different from our actual voice sometimes?\n",
      "Question 2: Why does my voice sound different from when I hear myself speak than it does on recordings?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 49 Pair:\n",
      "Question 1: Why do bridges become icy/freeze before the regular roads do?\n",
      "Question 2: Why are bridges more likely to freeze than a normal patch of the road?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 50 Pair:\n",
      "Question 1: Why so many Native American tribes seem to own casinos\n",
      "Question 2: Why are \"native American\" casinos a thing.\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 51 Pair:\n",
      "Question 1: Why are alcoholic beverages not required to list nutrition facts like everything else?\n",
      "Question 2: Why doesn't alcohol require \"nutritional facts\"?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 52 Pair:\n",
      "Question 1: Why are there people who are naturally ambidextrous?\n",
      "Question 2: What causes people to be ambidextrous?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 53 Pair:\n",
      "Question 1: Why does hair turn gray?\n",
      "Question 2: Why does our hair turn gray when we become older?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 54 Pair:\n",
      "Question 1: Why tipping in the United States seems to be more commonplace and at a higher percentage that other countries?\n",
      "Question 2: Why is tipping in USA so common, yet in the UK for example, it isnâ€™t?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 55 Pair:\n",
      "Question 1: Why tickling makes us laugh?\n",
      "Question 2: Why do we laugh when being tickled when it's not enjoyable whatsoever?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 56 Pair:\n",
      "Question 1: Why do animals freak out over laser pointers? (Cats especially)\n",
      "Question 2: Why do cats/dogs chase laser pointers?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 57 Pair:\n",
      "Question 1: With a carbonated drink in a glass, why do the bubbles appear in the places they do (i.e. in a seemingly random point on the side of the glass).\n",
      "Question 2: Why do bubbles appear from specific points around the side of the glass when it has a fizzy drink poured into it?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 58 Pair:\n",
      "Question 1: How do plants \"know\" where the Sun is?\n",
      "Question 2: How do plants \"know\" which way the sun is facing?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 59 Pair:\n",
      "Question 1: Why do couples sleep in the same bed?\n",
      "Question 2: Why do we sleep in the same bed as our partner?\n",
      "Similarity Score: 0.9010\n",
      "\n",
      "Top 60 Pair:\n",
      "Question 1: Why are there no \"Good\" diseases?\n",
      "Question 2: why are there virtually no \"good\" diseases or defects?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 61 Pair:\n",
      "Question 1: Where does gravity gets its energy?\n",
      "Question 2: Where does the energy that creates gravity come from?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 62 Pair:\n",
      "Question 1: What would really happen to an astronaut if they removed their helmet in space?\n",
      "Question 2: What would exactly happen if an astronaut on the moon would remove his helmet?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 63 Pair:\n",
      "Question 1: How are bridges over water built?\n",
      "Question 2: How are/were bridges built over large bodies of water, both now and before modern technologies?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 64 Pair:\n",
      "Question 1: How do microwaves heat food up?\n",
      "Question 2: How do microwaves cook food?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 65 Pair:\n",
      "Question 1: Why do birds bob their heads as they walk?\n",
      "Question 2: why do birds, like pigeons or crows, bob their heads when they walk? Why is that trait beneficial for them?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 66 Pair:\n",
      "Question 1: Why is it that our voice sounds different when we hear it from a video or recording versus how you hear it through your own ears?\n",
      "Question 2: Why does hearing your own voice sound different to when somebody else hears it, or when you listen to it on video?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 67 Pair:\n",
      "Question 1: Can the existence of randomness be proven?\n",
      "Question 2: Can something be proven as random?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 68 Pair:\n",
      "Question 1: What happens exactly when you're \"zoning out\"?\n",
      "Question 2: How does â€œzoning outâ€ really work?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 69 Pair:\n",
      "Question 1: How does your brain decide which memories to forget?\n",
      "Question 2: How does your brain decide what memories are important enough to remember and what memories to forget.\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 70 Pair:\n",
      "Question 1: Why does sleeping earlier (e.g.10pm-6am) feel more refreshing than sleeping later (e.g. 2am-10am) even though you are getting the same amount of sleep?\n",
      "Question 2: Why is it that sleeping from 10:30pm-6:30am feels more refreshing than from 12:00am-8:00am?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 71 Pair:\n",
      "Question 1: Why do fingers/toes hurt so much more when they are cold?\n",
      "Question 2: Why do your fingers hurt when it's cold?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 72 Pair:\n",
      "Question 1: Why is there thunder and lightning during rainstorms, but not during snowstorms?\n",
      "Question 2: Snow is precipitation, why does it not thunder and lightning like it does when it is raining?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 73 Pair:\n",
      "Question 1: Why do our voices sound weird to ourselves when we hear them recorded, but not to others?\n",
      "Question 2: why do our recorded voices sound different to us but normal to everyone else?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 74 Pair:\n",
      "Question 1: why is it your hands hurt more when you're cold?\n",
      "Question 2: Why do your fingers hurt when it's cold?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 75 Pair:\n",
      "Question 1: I've heard since you're constantly losing and regenerating cells about every 7 years you have a completely new body. If this is true how are tattoos permanent?\n",
      "Question 2: Exactly how do tattoos stay permanent when your body is constantly regenerating new cells?\n",
      "Similarity Score: 0.9005\n",
      "\n",
      "Top 76 Pair:\n",
      "Question 1: How are nutrition contents in food calculated?\n",
      "Question 2: How are the amounts of contents in food measured on the Nutrition Facts?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 77 Pair:\n",
      "Question 1: Why is music/lyrics so much easier to memorize than other things?\n",
      "Question 2: Why does music make it easier to memorize things?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 78 Pair:\n",
      "Question 1: Why do massages feel better when someone else is doing them to us then when we do them to our selfs.\n",
      "Question 2: why does it feel good when someone else massages us but not ourselves?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 79 Pair:\n",
      "Question 1: why do some animals have only one baby and some can have multiple?\n",
      "Question 2: Why do most animals have many babies at once but humans usually only have one at a time?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 80 Pair:\n",
      "Question 1: Why do cold drinks feel so much colder after chewing mint gum?\n",
      "Question 2: Why does ice water feel so much colder when I'm chewing mint flavored gum?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 81 Pair:\n",
      "Question 1: Why does it keep getting colder after Winter Solstice?\n",
      "Question 2: Why does winter get colder after the solstice when days start to become longer?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 82 Pair:\n",
      "Question 1: How did civilizations that don't share a common language start to communicate?\n",
      "Question 2: When people of different civilizations met without a common language, how did they communicate?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 83 Pair:\n",
      "Question 1: How is the nfl allowed to be a \"non-profit\" organization?\n",
      "Question 2: How is the NFL still considered a non-profit organization when they make billions of dollars?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 84 Pair:\n",
      "Question 1: Is there a genetic distinction between \"races\"? Or are the variations between ethnicities the same as the variations among them?\n",
      "Question 2: Is there a genetic difference between the races?\n",
      "Similarity Score: 0.9000\n",
      "\n",
      "Top 85 Pair:\n",
      "Question 1: How does a solar eclipse occur?\n",
      "Question 2: How does a solar eclipse work?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 86 Pair:\n",
      "Question 1: Why does lines of light appear when I squint my eyes while looking at a lamp?\n",
      "Question 2: Why do bright lights become lines when i squint my eyes?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 87 Pair:\n",
      "Question 1: Why does my voice sound so different in recordings?\n",
      "Question 2: Why does our normal voice sound different than our voice from a recording?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 88 Pair:\n",
      "Question 1: What causes that \"being watched\" feeling?\n",
      "Question 2: Can someone explain that feeling of \"being watched\", even when alone?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 89 Pair:\n",
      "Question 1: how milk becomes so many different types of cheese.\n",
      "Question 2: if cheese comes from milk, how do we get so many types of cheeses?\n",
      "Similarity Score: 0.9004\n",
      "\n",
      "Top 90 Pair:\n",
      "Question 1: Why is it when I eat spicy food it \"burns twice\"? What makes it burn on the way out?\n",
      "Question 2: why does spicy food burn coming out too?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 91 Pair:\n",
      "Question 1: How do ant colonies survive heavy rains?\n",
      "Question 2: How does an ant colony survive a flood or even a long soaking rain?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 92 Pair:\n",
      "Question 1: Why do I often wake up a few seconds or minutes before my alarm goes off?\n",
      "Question 2: How is it that I manage to wake up 2-3 minutes before my alarm goes off every day?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 93 Pair:\n",
      "Question 1: The difference between mass and weight\n",
      "Question 2: If weight and mass are different, why are they used interchangeably?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 94 Pair:\n",
      "Question 1: Why can't magnets be used to create a perpetual motion device?\n",
      "Question 2: Why couldn't a perpetual motion machine based around magnets work?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 95 Pair:\n",
      "Question 1: How does a digital camera \"know\" when it is, or is not in focus?\n",
      "Question 2: How do cameras \"know\" they're in focus?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 96 Pair:\n",
      "Question 1: Why it is more common for men to go bald\n",
      "Question 2: How and Why so many men go Bald?\n",
      "Similarity Score: 0.9009\n",
      "\n",
      "Top 97 Pair:\n",
      "Question 1: When the tide goes out in lakes or oceans, where does the water go?\n",
      "Question 2: Where does the water go when the tide changes?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 98 Pair:\n",
      "Question 1: Why does food I normally like taste terrible when I'm sick?\n",
      "Question 2: Why does everything taste a whole lot worse when you're sick?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 99 Pair:\n",
      "Question 1: Why does fatigueness fade away after being awake for a longer period of time?\n",
      "Question 2: Why does tiredness fade away when you stay awake too much?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 100 Pair:\n",
      "Question 1: What's the deal with the \"student loan bubble\" and what exactly will happen when it pops?\n",
      "Question 2: What will exactly happen when the US student loan bubble pops? How severe will this impact the economy?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 101 Pair:\n",
      "Question 1: What is a citizens arrest?\n",
      "Question 2: What is a citizen's arrest? Under what circumstances can one be performed?\n",
      "Similarity Score: 0.9006\n",
      "\n",
      "Top 102 Pair:\n",
      "Question 1: How does a pressure cooker work and why is it needed?\n",
      "Question 2: How do pressure cookers work?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 103 Pair:\n",
      "Question 1: Why is it so much easier to get sick when it's cold outside?\n",
      "Question 2: Why is it easier to get sick in cold weather rather than warm weather?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 104 Pair:\n",
      "Question 1: Why do so many medicines seem to potentially lead to suicidal thoughts?\n",
      "Question 2: Why do so many prescription meds have a side effect of \"suicidal thoughts\"?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 105 Pair:\n",
      "Question 1: The holes in airplane windows\n",
      "Question 2: Why do airplane windows have little holes in them?\n",
      "Similarity Score: 0.9008\n",
      "\n",
      "Top 106 Pair:\n",
      "Question 1: If I tried to \"swim\" through outer space, would I be able to?\n",
      "Question 2: Could I swim in space?\n",
      "Similarity Score: 0.9002\n",
      "\n",
      "Top 107 Pair:\n",
      "Question 1: why is it okay to eat blue cheese, but not cheese which has grown mold?\n",
      "Question 2: How is blue cheese a safe \"mold\" to eat?\n",
      "Similarity Score: 0.9007\n",
      "\n",
      "Top 108 Pair:\n",
      "Question 1: What happens to the other gases such as nitrogen in the air that we breath in?\n",
      "Question 2: What happens to the Nitrogen we breath in?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 109 Pair:\n",
      "Question 1: Where do the eyelashes that end inside the eyes go?\n",
      "Question 2: Where do all the eyelashes go that you can't get out and disappear into your eye?\n",
      "Similarity Score: 0.9001\n",
      "\n",
      "Top 110 Pair:\n",
      "Question 1: What separates a cult from a religion?\n",
      "Question 2: What are the main differences between a religion and a cult?\n",
      "Similarity Score: 0.9003\n",
      "\n",
      "Top 111 Pair:\n",
      "Question 1: why would anyone deny climate change\n",
      "Question 2: Why is it some people still deny climate change?\n",
      "Similarity Score: 0.9008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the top 5 most similar question pairs\n",
    "for i, (idx1, idx2, score) in enumerate(filtered_pairs):\n",
    "    question1 = filtered_train_df['question'][idx1]\n",
    "    question2 = filtered_test_df['question'][idx2]\n",
    "    print(f\"Top {i+1} Pair:\")\n",
    "    print(f\"Question 1: {question1}\")\n",
    "    print(f\"Question 2: {question2}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1710f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filter_train_df=pd.read_csv('filter_train_df.csv')\n",
    "filter_validation_df=pd.read_csv('filter_validation_df.csv')\n",
    "filter_test_df=pd.read_csv('filter_test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c099dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train set size: 217636\n",
      "Filtered validation set size: 18501\n",
      "Filtered test set size: 20378\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtered train set size: {len(filter_train_df)}\")\n",
    "print(f\"Filtered validation set size: {len(filter_validation_df)}\")\n",
    "print(f\"Filtered test set size: {len(filter_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5b7683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above used SBERT to remove train,validationa and test dataset questions similiariyt higher than 0.8 on top of TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c800987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did humans start keeping track of their age?</td>\n",
       "      <td>This is more of a question for r/AskAnthropolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why are do most medications have HCl in it?</td>\n",
       "      <td>I don't know what the pharmacological role of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If conservatives want small government, then w...</td>\n",
       "      <td>Because conservatives don't really want small ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do veterinarians clean the site of injecti...</td>\n",
       "      <td>At the very least, cleaning injection sites is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How did caterpillars evolve the process of met...</td>\n",
       "      <td>The opposite happened.  Insects used to hatch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20373</th>\n",
       "      <td>What caused the switch from doctors doing home...</td>\n",
       "      <td>If you keep the doctor in one place, he can sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20374</th>\n",
       "      <td>Does this actually work and if so how? [math]</td>\n",
       "      <td>It does not work. When you divide both sides o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20375</th>\n",
       "      <td>Would Varangians in early Kievan Rus have prac...</td>\n",
       "      <td>It is believed that the Varangians in the Rus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>Why is California always a blue state? Why is ...</td>\n",
       "      <td>While large part of land are red areas, there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20377</th>\n",
       "      <td>Why does the fight or flight response not caus...</td>\n",
       "      <td>There's also a deeper stress response than fig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256515 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      When did humans start keeping track of their age?   \n",
       "1            Why are do most medications have HCl in it?   \n",
       "2      If conservatives want small government, then w...   \n",
       "3      Why do veterinarians clean the site of injecti...   \n",
       "4      How did caterpillars evolve the process of met...   \n",
       "...                                                  ...   \n",
       "20373  What caused the switch from doctors doing home...   \n",
       "20374      Does this actually work and if so how? [math]   \n",
       "20375  Would Varangians in early Kievan Rus have prac...   \n",
       "20376  Why is California always a blue state? Why is ...   \n",
       "20377  Why does the fight or flight response not caus...   \n",
       "\n",
       "                                                  answer  \n",
       "0      This is more of a question for r/AskAnthropolo...  \n",
       "1      I don't know what the pharmacological role of ...  \n",
       "2      Because conservatives don't really want small ...  \n",
       "3      At the very least, cleaning injection sites is...  \n",
       "4      The opposite happened.  Insects used to hatch ...  \n",
       "...                                                  ...  \n",
       "20373  If you keep the doctor in one place, he can sp...  \n",
       "20374  It does not work. When you divide both sides o...  \n",
       "20375  It is believed that the Varangians in the Rus ...  \n",
       "20376  While large part of land are red areas, there ...  \n",
       "20377  There's also a deeper stress response than fig...  \n",
       "\n",
       "[256515 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full=pd.concat([filter_train_df, filter_validation_df,filter_test_df])\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "577c7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove stopwords (you can use a library like nltk or sklearn's stop words)\n",
    "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "    return text\n",
    "\n",
    "# Example of preprocessing the data\n",
    "questions = df_full['question'] # Example data\n",
    "processed_questions = [preprocess(q) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b5690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = filter_test_df['question'] # Example data\n",
    "processed_questions = [preprocess(q) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe37afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinmei/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the questions into sentence embeddings\n",
    "question_embeddings = model.encode(processed_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60244bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters (e.g., 3 for Science, History, General)\n",
    "num_clusters = 3\n",
    "\n",
    "# Use KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(question_embeddings)  # Use TF-IDF matrix if you're not using embeddings\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2236b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [ 57402  49931 149182]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get unique labels and their counts\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2c24154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['labels']=labels\n",
    "df_full_narrow=df_full[df_full['labels']!=2]\n",
    "df_full_narrow.to_csv('df_full_narrow.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a6a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_train_df['label'] = labels\n",
    "\n",
    "train_filtered = filter_train_df[filter_train_df['label'] != 0]\n",
    "\n",
    "# Drop the 'label' column if not needed\n",
    "train_filtered = train_filtered.drop(columns=['label'])\n",
    "train_filtered.to_csv('train_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_validation_df['label'] = labels\n",
    "\n",
    "validation_filtered = filter_validation_df[filter_validation_df['label'] != 0]\n",
    "\n",
    "# Drop the 'label' column if not needed\n",
    "validation_filtered = validation_filtered.drop(columns=['label'])\n",
    "validation_filtered.to_csv('validation_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e85102c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_test_df['label'] = labels\n",
    "\n",
    "test_filtered = filter_test_df[filter_test_df['label'] != 0]\n",
    "\n",
    "# Drop the 'label' column if not needed\n",
    "test_filtered = test_filtered.drop(columns=['label'])\n",
    "test_filtered.to_csv('test_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc540862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did humans start keeping track of their age?</td>\n",
       "      <td>This is more of a question for r/AskAnthropolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How fast did news of political developments tr...</td>\n",
       "      <td>It would take about a month for official lette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does rain change intensity when it falls e...</td>\n",
       "      <td>Over short periods like minutes, it just depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>If light and radio waves are both electromagne...</td>\n",
       "      <td>Well, you don't really need to \"illuminate\" th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the distribution of photon like?</td>\n",
       "      <td>It is simply that there are so many photons th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217626</th>\n",
       "      <td>Why hasn't the asteroid belt coalesed into a m...</td>\n",
       "      <td>You could fly through the asteroid belt with y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217627</th>\n",
       "      <td>How accurate is Orwell's description of the po...</td>\n",
       "      <td>Recently there was a interesting article in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217630</th>\n",
       "      <td>How do UN peacekeeping forces work</td>\n",
       "      <td>They generally go in and separate groups in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217633</th>\n",
       "      <td>How much hydrogen actually undergoes fusion in...</td>\n",
       "      <td>It depends on the weapon design, and they do v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217635</th>\n",
       "      <td>Military escorts of passenger jets</td>\n",
       "      <td>When a terrorist seizes control of a plane, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90848 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "0       When did humans start keeping track of their age?   \n",
       "5       How fast did news of political developments tr...   \n",
       "7       How does rain change intensity when it falls e...   \n",
       "11      If light and radio waves are both electromagne...   \n",
       "13               What is the distribution of photon like?   \n",
       "...                                                   ...   \n",
       "217626  Why hasn't the asteroid belt coalesed into a m...   \n",
       "217627  How accurate is Orwell's description of the po...   \n",
       "217630                 How do UN peacekeeping forces work   \n",
       "217633  How much hydrogen actually undergoes fusion in...   \n",
       "217635                 Military escorts of passenger jets   \n",
       "\n",
       "                                                   answer  \n",
       "0       This is more of a question for r/AskAnthropolo...  \n",
       "5       It would take about a month for official lette...  \n",
       "7       Over short periods like minutes, it just depen...  \n",
       "11      Well, you don't really need to \"illuminate\" th...  \n",
       "13      It is simply that there are so many photons th...  \n",
       "...                                                   ...  \n",
       "217626  You could fly through the asteroid belt with y...  \n",
       "217627  Recently there was a interesting article in th...  \n",
       "217630  They generally go in and separate groups in co...  \n",
       "217633  It depends on the weapon design, and they do v...  \n",
       "217635  When a terrorist seizes control of a plane, th...  \n",
       "\n",
       "[90848 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469967cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2] [126788  42120  48728]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get unique labels and their counts\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(unique_labels, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee3cda53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do the time differences between planets wo...</td>\n",
       "      <td>It's a consequence of General Relativity. It's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Was medieval armour(14th -15th century) painte...</td>\n",
       "      <td>If I did this right [this] (_URL_0_) should be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A large body of water produces a tone when it'...</td>\n",
       "      <td>The sound of each water drop is not a pure ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how do posthumous albums sound like they are r...</td>\n",
       "      <td>those recordings were made while he was alive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is there a clear, simple history of the Homo g...</td>\n",
       "      <td>Hi, this is an anthropology question. There ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8669</th>\n",
       "      <td>Are new planets and stars currently being formed?</td>\n",
       "      <td>There are new planets and stars being formed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>Is it true that Goethe's book \"Die Leiden Des ...</td>\n",
       "      <td>Yes. It is true. It also coined the German ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>Why is it hard to find a star system with 9 or...</td>\n",
       "      <td>If you take our solar system as an example it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>What exactly does it mean for a territory to n...</td>\n",
       "      <td>They're like your dog. You totally consider hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>Would Varangians in early Kievan Rus have prac...</td>\n",
       "      <td>It is believed that the Varangians in the Rus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8674 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     how do the time differences between planets wo...   \n",
       "1     Was medieval armour(14th -15th century) painte...   \n",
       "2     A large body of water produces a tone when it'...   \n",
       "3     how do posthumous albums sound like they are r...   \n",
       "4     Is there a clear, simple history of the Homo g...   \n",
       "...                                                 ...   \n",
       "8669  Are new planets and stars currently being formed?   \n",
       "8670  Is it true that Goethe's book \"Die Leiden Des ...   \n",
       "8671  Why is it hard to find a star system with 9 or...   \n",
       "8672  What exactly does it mean for a territory to n...   \n",
       "8673  Would Varangians in early Kievan Rus have prac...   \n",
       "\n",
       "                                                 answer  \n",
       "0     It's a consequence of General Relativity. It's...  \n",
       "1     If I did this right [this] (_URL_0_) should be...  \n",
       "2     The sound of each water drop is not a pure ton...  \n",
       "3     those recordings were made while he was alive,...  \n",
       "4     Hi, this is an anthropology question. There ar...  \n",
       "...                                                 ...  \n",
       "8669  There are new planets and stars being formed a...  \n",
       "8670  Yes. It is true. It also coined the German ter...  \n",
       "8671  If you take our solar system as an example it ...  \n",
       "8672  They're like your dog. You totally consider hi...  \n",
       "8673  It is believed that the Varangians in the Rus ...  \n",
       "\n",
       "[8674 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `filter_test_df` is your DataFrame and `labels` is the array of cluster labels\n",
    "filter_test_df['cluster'] = labels  # Add the cluster labels to the DataFrame\n",
    "\n",
    "# Filter to keep only the rows where the cluster label is 1 or 2\n",
    "test_df_no_general = filter_test_df[filter_test_df['cluster'].isin([1, 2])]\n",
    "\n",
    "# Drop the 'cluster' column if you don't need it anymore\n",
    "test_df_no_general = test_df_no_general.drop(columns=['cluster'])\n",
    "\n",
    "# Reset the index (optional)\n",
    "test_df_no_general = test_df_no_general.reset_index(drop=True)\n",
    "\n",
    "# Check the result\n",
    "test_df_no_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d62d182a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why are laptops with SSDs always so space cons...</td>\n",
       "      <td>&gt; Like is there a downside to these phone stor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the oldest known case of Bi Polar Diso...</td>\n",
       "      <td>Like many illnesses, the ancient Greeks were p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why are broths and soups commonly consumed whe...</td>\n",
       "      <td>because when you have a cold your throat usual...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the second law of thermodynamics factor i...</td>\n",
       "      <td>The laws of thermodynamics are laws for thermo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Difference in body temp vs what someone else p...</td>\n",
       "      <td>First, it's because the subjective experience ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does the ISS avoid damage from solar wind'...</td>\n",
       "      <td>The ISS orbits beneath the protective shield c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can we put an estimated time until death o...</td>\n",
       "      <td>we can estimate/calculate the mass of the sun....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why do small, out of focus objects always show...</td>\n",
       "      <td>They are deformed by the camera's aperture. Of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What happens when you descend into a gas giant?</td>\n",
       "      <td>&gt; is it possible to eventually reach a point w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Would neutron material be stable without the g...</td>\n",
       "      <td>Neutrons have an approximately 10 minute half-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why did China not have a warrior class and cod...</td>\n",
       "      <td>Why do you expect there to be? Is there any re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Suppose you're an English man half way through...</td>\n",
       "      <td>_URL_1_ _URL_0_ It seems that for the Army at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>During WWII, what was the public reaction to t...</td>\n",
       "      <td>If you haven't already, check out John Dower's...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Index of Economic Freedom and how/why Hong Kon...</td>\n",
       "      <td>_URL_0_ It ranks how much a country protects p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How open were previous civilizations to childr...</td>\n",
       "      <td>Going to throw this out, though it's by nature...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Why are laptops with SSDs always so space cons...   \n",
       "1   What is the oldest known case of Bi Polar Diso...   \n",
       "2   Why are broths and soups commonly consumed whe...   \n",
       "3   Does the second law of thermodynamics factor i...   \n",
       "4   Difference in body temp vs what someone else p...   \n",
       "5   How does the ISS avoid damage from solar wind'...   \n",
       "6   How can we put an estimated time until death o...   \n",
       "7   Why do small, out of focus objects always show...   \n",
       "8     What happens when you descend into a gas giant?   \n",
       "9   Would neutron material be stable without the g...   \n",
       "10  Why did China not have a warrior class and cod...   \n",
       "11  Suppose you're an English man half way through...   \n",
       "12  During WWII, what was the public reaction to t...   \n",
       "13  Index of Economic Freedom and how/why Hong Kon...   \n",
       "14  How open were previous civilizations to childr...   \n",
       "\n",
       "                                               answer  cluster  \n",
       "0   > Like is there a downside to these phone stor...        0  \n",
       "1   Like many illnesses, the ancient Greeks were p...        0  \n",
       "2   because when you have a cold your throat usual...        0  \n",
       "3   The laws of thermodynamics are laws for thermo...        0  \n",
       "4   First, it's because the subjective experience ...        0  \n",
       "5   The ISS orbits beneath the protective shield c...        1  \n",
       "6   we can estimate/calculate the mass of the sun....        1  \n",
       "7   They are deformed by the camera's aperture. Of...        1  \n",
       "8   > is it possible to eventually reach a point w...        1  \n",
       "9   Neutrons have an approximately 10 minute half-...        1  \n",
       "10  Why do you expect there to be? Is there any re...        2  \n",
       "11  _URL_1_ _URL_0_ It seems that for the Army at ...        2  \n",
       "12  If you haven't already, check out John Dower's...        2  \n",
       "13  _URL_0_ It ranks how much a country protects p...        2  \n",
       "14  Going to throw this out, though it's by nature...        2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_questions = []\n",
    "for cluster_num in range(num_clusters):\n",
    "    # Filter the DataFrame for the current cluster\n",
    "    cluster_df = filter_test_df[filter_test_df['cluster'] == cluster_num]\n",
    "    \n",
    "    # Randomly sample 5 questions from this cluster\n",
    "    # Use min() to avoid errors if there are fewer than 5 questions in the cluster\n",
    "    sampled = cluster_df.sample(n=min(5, len(cluster_df)), random_state=42)\n",
    "    sampled_questions.append(sampled)\n",
    "\n",
    "# Concatenate the samples from all clusters\n",
    "sampled_df = pd.concat(sampled_questions)\n",
    "\n",
    "# Reset index for clarity\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sampled questions\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ff92cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Would neutron material be stable without the gravity of a star bearing down on it?\n",
      "A: Neutrons have an approximately 10 minute half-life before they decay into a proton, an electron, and an anti-neutrino if they have not yet been absorbed by a nucleus. If they are in the gravitational field of a neutron star, I am guessing the gravitational field would just force them back together. I am guessing that the mass and volume of the teaspoon is not the \"right\" amount to produce a gravitational field to overcome the electric repulsion of the protons and electrons. After enough time, you may just have a bunch of hydrogen. I am just an engineer though, a physicist or astro-physicist will give you a much better answer.\n",
      "\n",
      "Q: Why are laptops with SSDs always so space constrained?\n",
      "A: > Like is there a downside to these phone storage options I don't know about like insufficient speed or read/write limits That's *exactly* the difference. SSDs are far faster & more durable than the storage in a phone or on an SD card. This makes them cost significantly more.\n",
      "\n",
      "Q: Why did China not have a warrior class and code (like samurai/bushido, knights/chivalry, etc)?\n",
      "A: Why do you expect there to be? Is there any reason having a \"warrior class\" is the norm, and deviations need to be explained? That aside, comparing China to Medieval Europe and Japan, which were both fractured into non centralized polities, isn't very helpful. When Japan centralized during the Edo period, for example, the samurai essentially became analogous to the Chinese *shi* scholar-officials, and (say) sixteenth century European warfare isn't exactly known for its heavy reliance on feudal, martial aristocracies. Likewise, you don't really have \"knights\" in Rome, or even the Carolingian period. Martial aristocracies are largely the product of fragmented political conditions.\n",
      "\n",
      "Q: What is the oldest known case of Bi Polar Disorder, specifically manic episodes?\n",
      "A: Like many illnesses, the ancient Greeks were pretty damn good at figuring things out. That goes for the ancient world in general: Greek, Roman, Persian, Chinese, Egyptian, etc. scholars described mental illnesses quite accurately. Many indigenous cultures also recognize specific mental illnesses. It was when mental illnesses started to get wrapped in religion and moral values that things started to go sideways and people looked to religion for treatment. Mental illnesses like bipolar disorder don't cause death on their own. It's not like your brain falls apart. Bipolar disorder is an episodic illness. If one doesn't receive treatment, the episode will eventually come to an end on its own. Some people may only have a few episodes of depression and mania over the course of their lifetime. Other people have a lot of episodes.\n",
      "\n",
      "Q: Difference in body temp vs what someone else perceives as your body temp\n",
      "A: First, it's because the subjective experience of temperature is not the same thing as actual temperature.  Second but related, our personal experience of \"how hot/cold I am\" depends on the RATE at which we lose/gain heat. If you're losing heat, you feel cold.  So when your SO is losing heat, they may interpret this as feeling cold. When you touch them (and if your skin temperature is less than theirs), you absorb their heat and interpret this as them being warm since you received energy from them.  The actual physical temperature of the skin plays a smaller role in all of this than you would expect.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample 350 rows from your test dataset\n",
    "\n",
    "sample_indices = random.sample(range(len(sampled_df)), 5)\n",
    "sampled_questions = [sampled_df['question'][i] for i in sample_indices]\n",
    "sampled_answers = [sampled_df['answer'][i] for i in sample_indices]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Q: {sampled_questions[i]}\")\n",
    "    print(f\"A: {sampled_answers[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e381d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from umap-learn) (1.7.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from umap-learn) (4.66.5)\n",
      "Requirement already satisfied: numba>=0.51.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from umap-learn) (0.54.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from umap-learn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from umap-learn) (1.20.3)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from numba>=0.51.2->umap-learn) (58.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31e633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinmei/.local/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "# Apply UMAP for dimensionality reduction\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = reducer.fit_transform(question_embeddings)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.title('UMAP Visualization of Clusters')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f05bdaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 questions:\n",
      "['limbs try automatically copy im watching action scene'\n",
      " 'genespliced foods safe severe allergies'\n",
      " 'immunologists autoimmune disorder resistent normal infections'\n",
      " 'mass exodus mormon church thats refusing homosexuals'\n",
      " 'moviestv clear cut lawsuit company â€œthey million dollar lawyers tie court bankrupt feesâ€ reality versus hollywood']\n",
      "Cluster 1 questions:\n",
      "['time differences planets work interstellar'\n",
      " 'large body water produces tone hit rain frequency tone'\n",
      " 'earths rotational speed slowing' 'propane gases ground'\n",
      " 'molecular lever metals shiny nonmetals']\n",
      "Cluster 2 questions:\n",
      "['medieval armour14th 15th century painted means maintenance rust prevention paint consist thanks'\n",
      " 'posthumous albums sound like recorded artist alive'\n",
      " 'clear simple history homo genus' 'pros cons semieu countries'\n",
      " 'genesis socalled hitler youth haircut nazi original simply associate popular culture']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the first few questions in each cluster\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_questions = np.array(processed_questions)[labels == cluster_num]\n",
    "    print(f\"Cluster {cluster_num} questions:\")\n",
    "    print(cluster_questions[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc857361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us fine tune Llama with new Eli5 which has very limited overlap between train, validation and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6dffc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(filter_train_df)\n",
    "validation_dataset = Dataset.from_pandas(filter_validation_df)\n",
    "test_dataset = Dataset.from_pandas(filter_test_df)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43206841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 217636\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 18501\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 20378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2476e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145dd4bf74644190a6299311e9aa7c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50b82a83e9243069a7897f8341e4630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18501 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6152395960d42edb8cbc262e8312e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(dataset['train'])\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10218c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e89713d83b54c2fb985e6fba684f083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/217636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb9093dee65456b8ae47852b2fd0cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/20378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 3000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        eval_strategy=\"steps\",  # You can also use 'epoch' if preferred\n",
    "        eval_steps=250,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9242d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3c6fc639340afb60bc3e2870b67cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f60e836e1848a6b19b9493f5b0fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b5bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8df8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
      "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 217,636 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 64 | Total steps = 3,000\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 2:57:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.852000</td>\n",
       "      <td>2.753301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.723000</td>\n",
       "      <td>2.735124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.677900</td>\n",
       "      <td>2.724913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.636700</td>\n",
       "      <td>2.716450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.720700</td>\n",
       "      <td>2.709903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.546200</td>\n",
       "      <td>2.704884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.649500</td>\n",
       "      <td>2.700153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.697300</td>\n",
       "      <td>2.696063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.718000</td>\n",
       "      <td>2.692770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.707900</td>\n",
       "      <td>2.690307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.600600</td>\n",
       "      <td>2.688404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.676400</td>\n",
       "      <td>2.687581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802fda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = trainer.model\n",
    "output_dir = \"C:/Users/luan/Desktop/peft_weights_short\"\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb537dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/luan/Desktop/peft_weights_short/tokenizer_config.json',\n",
       " 'C:/Users/luan/Desktop/peft_weights_short/special_tokens_map.json',\n",
       " 'C:/Users/luan/Desktop/peft_weights_short/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c453760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "\n",
    "# Load the PEFT configuration\n",
    "peft_model_id = \"C:/Users/luan/Desktop/peft_weights_short\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# Load the tokenizer and processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b85711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3a520b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : The difference between a learning disability and an intellectual disability\n",
      "Model Output: The difference between a learning disability and an intellectual disability is often misunderstood. A person with an intellectual disability may have average to below-average intelligence, but still be able to learn new things through practice or repetition (e.g., someone who has trouble remembering their name). On the other hand, a person with a learning disability might struggle even when they try very hard to remember information - it's just that he/she can't retain any of what was learned.\n",
      "\n",
      "Here are some key differences in how these two conditions manifest: *   **Intellectual Disability:**  > In most cases, people with intellectual disabilities do not develop speech difficulties because language skills were never fully developed as children. They will generally use gestures, sign language, pictures, or writing symbols instead for communication. *   **Learning Disabilities:** Learning disabilities involve difficulty acquiring knowledge based on reading comprehension and/or processing written material. These problems usually result from issues related to memory, attention span, concentration, or understanding complex concepts. The causes of these disorders vary widely depending upon the specific condition being described.\n",
      "----------\n",
      "Base Model Output: The difference between a learning disability and an intellectual disability is not well defined. The terms are often used interchangeably, but they have different meanings.\n",
      "Intellectual disabilities refer to cognitive impairments that affect thinking or reasoning in general. These can include difficulties with memory, attention span, problem-solving skills, language use (including reading, writing, speaking), social interaction, and other functions necessary for daily life. People who have intellectual disabilities may also be considered mentally retarded if their IQ scores fall below 70-75 on standardized tests of intelligence.\n",
      "\n",
      "Learning Disabilities: Learning disabilities involve problems related specifically to the way information is processed by your brain. They don't necessarily relate directly to any specific function you need to perform every day; instead, they might cause difficulty remembering certain types of material, such as math concepts, spelling words correctly, or following instructions. There are many kinds of learning disordersâ€”some are more common than othersâ€”and some people learn them at varying rates depending upon how much practice they get over time. While there's no single test for determining whether someone\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question=\"The difference between a learning disability and an intellectual disability\"\n",
    "inputs = tokenizer(text=question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "    base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "base_generated_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "print(f\"Question : {question}\")\n",
    "print(f\"Model Output: {generated_text}\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"Base Model Output: {base_generated_text}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dd30249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assuming your test dataset is a list called 'test_data'\n",
    "sampled_data = random.sample(dataset['test']['question'], 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee850ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:19<00:00, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 350 answers from the fine-tuned model.\n",
      "Generated 350 answers from the baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 10  # Start with a small batch size\n",
    "all_generated_texts = []\n",
    "all_base_generated_texts = []\n",
    "\n",
    "for i in tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    # Get the current batch of questions\n",
    "    batch_questions = sampled_data[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        # Tokenize and move to GPU\n",
    "        inputs = processor(text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "            base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "        generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "        base_generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in base_outputs]\n",
    "\n",
    "        # Collect the results\n",
    "        all_generated_texts.extend(generated_texts)\n",
    "        all_base_generated_texts.extend(base_generated_texts)\n",
    "\n",
    "        # Clear cache after each batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM error. Consider reducing batch size.\")\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"Generated {len(all_generated_texts)} answers from the fine-tuned model.\")\n",
    "print(f\"Generated {len(all_base_generated_texts)} answers from the baseline model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7be9ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Why do the f1 cars have some sort of air blowing motor attached to them on the starting grid?\n",
      "Model Output: Why do the f1 cars have some sort of air blowing motor attached to them on the starting grid? This is called a starter and it helps get their engines running. It's also why they don't start in circles, because if you're already revving your engine at 1000 RPM when someone else starts right next to you then there will be no point trying to pass each other as both are just going to stall before anyone can even make any progress.\n",
      "I've always wondered what kind of technology would need to go into that system so I could replicate this without having to physically attach an external unit or something like that? Or maybe use one existing setup from another car which has been modified for racing purposes? The reason i'm curious about all this is since F1 drivers actually wear helmets with built-in sensors and computers that monitor everything around them while driving (including speed, acceleration, g-forces etc) but not much information seems available online regarding how these systems work internally.\n",
      "----------\n",
      "Base Model Output: Why do the f1 cars have some sort of air blowing motor attached to them on the starting grid? This is called a \"wind tunnel\" and it helps keep their tires from sticking too much when they're first rolling off. I've seen pictures where you can see this in action, but how does that actually work?\n",
      "I'm curious about all these different types of wind tunnels used for testing F1 cars. What are there? There's no real-world racing going on here!\n",
      "--------------------------------------------------\n",
      "Question 2: Genetic drift in cultured bacteria?\n",
      "Model Output: Genetic drift in cultured bacteria? No, it's just a matter of time and conditions. This is what happens when you have too many cells (in this case 1000s) that are all growing at the same rate without any selective pressure to change their behavior or structure.\n",
      "This means they will continue on their original path until something else comes along like a mutation which changes them into a different form but still keeps them alive since there isn't enough selection for one particular trait to become more dominant over another. In other words, if I were to put these cells back together with some new ones from a completely separate colony, after several generations would we see noticeable differences between those two colonies as well? Yes, probably not unless they get exposed to an external stressor such as radiation or chemicals that can cause mutations. The vast majority of genetic variation occurs through random chance so even though there may be no natural selection acting upon the population, the number of individuals surviving could eventually lead to small variations in traits due to random events happening across\n",
      "----------\n",
      "Base Model Output: Genetic drift in cultured bacteria? I mean, they're not like a wild animal or anything. But if you look at the distribution of bacterial species and how many different strains are found around us it seems to be pretty random with no clear pattern.\n",
      "What about when we go into an environment that has been altered by human activity (e.g., urbanization) but still have some natural biodiversity present? We see lots of other things going on there too... What is happening to these new \"cultivated\" environments? Is this just genetic drift occurring as well?\n",
      "\n",
      "I'm curious what happens to our microbial populations over time!\n",
      "--------------------------------------------------\n",
      "Question 3: Why do some artists that make really good art feel that there work is terrible/not good enough?\n",
      "Model Output: Why do some artists that make really good art feel that there work is terrible/not good enough? They are just so used to making their own stuff and not being able to get it out into the world, they can't stand criticism of anything. It's a bit like if you were in school for 10 years and never got any recognition or awards but still felt bad about yourself because everyone else was doing better than you. This isn't healthy at all! You need people telling your worth as an artist (and anyone) and this will help keep you motivated to continue creating something new every day.\n",
      "----------\n",
      "Base Model Output: Why do some artists that make really good art feel that there work is terrible/not good enough? They are often the ones who have a high level of skill and talent. This can be attributed to their own self doubt, lack of confidence in themselves or other people's opinions on how they should create something new. I am sure this is true for many if not most creative types out there! But what about when someone makes an amazing piece but feels like it was made by others instead of them? Do they also experience similar feelings?\n",
      "--------------------------------------------------\n",
      "Question 4: Why did China recieve Veto-power at the creation of the SC of the UN?\n",
      "Model Output: Why did China recieve Veto-power at the creation of the SC of the UN? The US and UK had veto power, but they were both major world powers with significant influence in international affairs. Why didn't China get a say?\n",
      "The answer lies not only within their own history (China's rise to superpower status), but also because of how the United Nations was established after WW2. Prior to WWII there wasn't an organization that could represent all nations on earth under one roof. After 1945 the U.N. came into existence as an attempt to bring together countries which would otherwise be too powerful or wouldn't cooperate due to past conflicts. This led to several smaller organizations being formed around various regions such as Europe and Asia.\n",
      "\n",
      "It is interesting though why this happened since it seems like the Chinese government has been trying for years now to join them. They've tried joining through diplomatic channels before but have always failed due to disputes over issues related to sovereignty, economic interests etc. So I guess you can see two reasons here - first the lack of representation by other large players means\n",
      "----------\n",
      "Base Model Output: Why did China recieve Veto-power at the creation of the SC of the UN? The US and UK were part of the 5 permanent members, while Japan was added after WWII. Why wasn't it a veto power for all countries in general?\n",
      "The reason is that only 2 states (the USA & USSR) had their own veto powers before the end of WWI. Both had been granted such by the Paris Peace Conference which ended the war. So there are two reasons: \n",
      "1- It's not as if any other country would have wanted to upset either side so much as they already held significant influence over both sides. A third party might try but be deterred due to fear of retaliation from one or both parties.\n",
      "2- As mentioned above, most countries do not want to get involved with an international body like this because having your vote counted against another nation can result in severe consequences - even death. No matter how well you think about something, someone will always find ways to make sure you don't see its benefits without contributing to it.\n",
      "--------------------------------------------------\n",
      "Question 5: How did an average person get the news before the newspaper and printing press were invented? How much would he actually know of big or important current events?\n",
      "Model Output: How did an average person get the news before the newspaper and printing press were invented? How much would he actually know of big or important current events? News was often passed from word-of-mouth, rumors, gossip, letters to relatives, newspapers (which didn't really exist until after 1665), etc. The way people communicated information around town varied greatly depending on their location, social class, occupation, and even time period.\n",
      "\n",
      "The only thing I can say is that in my experience as a high school student living during this era it seems like most people had little access to reliable sources for accurate reporting - they relied mostly on what they heard at home with family members who might have gotten some info second-hand through someone else's network. It wasn't uncommon for me to hear about something happening somewhere but not be able to find out where exactly because there weren't many newspapers around...and if you're talking about major international issues then yes, by no means everyone knew everything!\n",
      "----------\n",
      "Base Model Output: How did an average person get the news before the newspaper and printing press were invented? How much would he actually know of big or important current events? Was it limited to what was reported in print, radio broadcasts, newspapers (and later TV) and telephones?\n",
      "\n",
      "I'm particularly interested how different regions affected by their geography had varying levels of access. Were there certain areas where people could only hear about things happening through word-of-mouth, local media like churches, community gatherings etc., while others had more formalized ways of getting information from outside sources such as government reports, international news agencies...or other forms of mass communication that weren't available everywhere.\n",
      "\n",
      "Thanks! I'd love some historical examples if you can find any.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Question {i+1}: {sampled_data[i]}\")\n",
    "    print(f\"Model Output: {all_generated_texts[i]}\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"Base Model Output: {all_base_generated_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67223868",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3303207/2026646992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a DataFrame for easier analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m output_df_short = pd.DataFrame({\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"baseline_answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_base_generated_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"fine_tuned_answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_generated_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampled_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "output_df_short = pd.DataFrame({\n",
    "    \"question\": sampled_data,\n",
    "    \"baseline_answer\": all_base_generated_texts,\n",
    "    \"fine_tuned_answer\": all_generated_texts\n",
    "})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df_short.to_csv(\"generated_answers_short.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df_short=pd.read_csv(\"generated_answers_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ffc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc2578a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:02,  9.59it/s]/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.4507\n",
      "Average ROUGE-2 F1: 0.1737\n",
      "Average ROUGE-L F1: 0.2571\n",
      "Average BLEU Score: 0.1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = evaluate_in_batches(\n",
    "    output_df_short['fine_tuned_answer'], output_df_short['baseline_answer'], batch_size=10\n",
    ")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#dataset with history and science topcis only\n",
    "test_filtered=pd.read_csv('test_filtered.csv')\n",
    "train_filtered=pd.read_csv('train_filtered.csv')\n",
    "validation_filtered=pd.read_csv('validation_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55681928",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_full=pd.concat([test_filtered,validation_filtered,train_filtered]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7e8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_full.to_csv(\"filtered_full.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c667589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_filtered)\n",
    "validation_dataset = Dataset.from_pandas(validation_filtered)\n",
    "test_dataset = Dataset.from_pandas(test_filtered)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25ac862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 90848\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 8167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 8674\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81196ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f635d755c8f4ba9abbde61645b4292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140b5f88f0504c26a4908bafea8d2c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75b1d0b33ba4c5aa0aabea0a76c3129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "#dataset = standardize_sharegpt(dataset['train'])\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2b719e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e8b718a9e145a2abfa3dc7bc962db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/90848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e24a2044a5e448d8eea4798eb979a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/8674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 3000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        eval_strategy=\"steps\",  # You can also use 'epoch' if preferred\n",
    "        eval_steps=250,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caee8230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5760e6b2469948319e520a7d9617ee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b363f1cc14844acf87ba9e582701f286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8674 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190ac10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
      "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 90,848 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 64 | Total steps = 3,000\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1512' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1512/3000 1:17:33 < 1:16:25, 0.32 it/s, Epoch 1.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.717600</td>\n",
       "      <td>2.739878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.756600</td>\n",
       "      <td>2.718322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.741700</td>\n",
       "      <td>2.705191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.533300</td>\n",
       "      <td>2.696186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.748700</td>\n",
       "      <td>2.687858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.572300</td>\n",
       "      <td>2.684886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c388098",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = trainer.model\n",
    "output_dir = \"C:/Users/luan/Desktop/peft_weights_narrow\"\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240ff976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "\n",
    "# Load the PEFT configuration\n",
    "peft_model_id = \"C:/Users/luan/Desktop/peft_weights_narrow\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# Load the tokenizer and processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeac1f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fd9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assuming your test dataset is a list called 'test_data'\n",
    "sampled_data = random.sample(dataset['test']['question'], 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09264ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:01<00:00, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 350 answers from the fine-tuned model.\n",
      "Generated 350 answers from the baseline model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "batch_size = 10  # Start with a small batch size\n",
    "all_generated_texts = []\n",
    "all_base_generated_texts = []\n",
    "\n",
    "for i in tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    # Get the current batch of questions\n",
    "    batch_questions = sampled_data[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        # Tokenize and move to GPU\n",
    "        inputs = processor(text=batch_questions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "            base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id \n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "        generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "        base_generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in base_outputs]\n",
    "\n",
    "        # Collect the results\n",
    "        all_generated_texts.extend(generated_texts)\n",
    "        all_base_generated_texts.extend(base_generated_texts)\n",
    "\n",
    "        # Clear cache after each batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"CUDA OOM error. Consider reducing batch size.\")\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"Generated {len(all_generated_texts)} answers from the fine-tuned model.\")\n",
    "print(f\"Generated {len(all_base_generated_texts)} answers from the baseline model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de08388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Do we have any evidence for prehistoric civilizations or states?\n",
      "Model Output: Do we have any evidence for prehistoric civilizations or states? There are some interesting examples of ancient cities and towns, but I'm not sure if they qualify as \"civilizations\" in the modern sense. For example, could a city like Pompeii be considered one?\n",
      "\n",
      "I know that there were many cultures before written records existed (the Bronze Age) so it's possible to make educated guesses about what people lived during those times. However, how accurate is this guesswork based on oral tradition and archaeological findings? Do you think our understanding has improved over time due to new discoveries, better technology, etc.?\n",
      "----------\n",
      "Base Model Output: Do we have any evidence for prehistoric civilizations or states? The answer is a resounding yes. There are several examples of large-scale, organized societies that existed before the rise of modern civilization.\n",
      "\n",
      "One example I would like to point out in particular is the [Indus Valley Civilization](_URL_0_), which flourished around 3300-1900 BCE and was located on the Indus River basin between present-day Pakistan and northwestern India. It's known primarily through archaeological remains, but there were cities with advanced architecture, water management systems, writing (cuneiform), and trade networks stretching from China to Egypt and Europe. This civilization had its own written language, administrative system, religious practices, art style, and even an early form of governance structure. While it wasn't exactly what you're looking for, it certainly has some interesting aspects!\n",
      "--------------------------------------------------\n",
      "Question 2: Was there a name for the Battlefield in RagnarÃ¶k\n",
      "Model Output: Was there a name for the Battlefield in RagnarÃ¶k? The Battle of Valhalla\n",
      "I'm just curious, is it true that after Ragnarok happens, Odin and his Einherjar go to Valhalla as well? Is this part of Norse mythology or was it later added by Christian writers like JÃ¶rmungandr's father Njord?\n",
      "How do you know about this if I don't have any sources at hand right now.\n",
      "----------\n",
      "Base Model Output: Was there a name for the Battlefield in RagnarÃ¶k? JÃ¶rmungandr's tail was said to be as long as 3000 years, and it would have covered an area of several hundred miles. The battle took place on this battlefield where he killed all his enemies with one blow from his hammer Mjolnir.\n",
      "\n",
      "In Norse mythology, the world ends at RagnarÃ¶k (Old Norse: Ragnarok) when gods and giants fight each other until only two are left standing - Odin and Loki or Thor and Balder. This is called \"the end\" because everything that lives dies before then. However, some people still believe that RagnarÃ¶k can't happen today due to how bad things seem now but if we keep fighting wars every day maybe someday something like that will occur again.\n",
      "--------------------------------------------------\n",
      "Question 3: If bacteria is everywhere and can seemingly grow on anything, then why aren't more colonies visible?\n",
      "Model Output: If bacteria is everywhere and can seemingly grow on anything, then why aren't more colonies visible? This question has been asked before. Here are some related threads: _URL_1_ _URL_0_ As you said yourself \"they're just invisible\", but they do exist in a very small amount at any given time (typically < 10^-8 m^3). The problem with the first two examples I mentioned was that we have no idea how many there really are or what their average size / density would be like. We don't know if all of them were actively growing to form larger clumps every day. They could potentially go dormant for years until reactivated when conditions become favorable again. That's also true about most microorganisms - only under specific circumstances will it get activated from its resting state. So even though you see something moving around, chances are those individual cells might not actually multiply much over the course of your lifetime due to factors such as low numbers, slow growth rates, competition etc... It doesn't mean these things aren't happening! But yes, they probably\n",
      "----------\n",
      "Base Model Output: If bacteria is everywhere and can seemingly grow on anything, then why aren't more colonies visible? This question has been asked before. I'm not sure what the answer would be but it seems like there are a few possibilities that could explain this: 1) The environment we live in isn't conducive to bacterial growth (e.g., high temperatures or low humidity). Bacteria need these conditions to survive and thrive. If our environments don't meet their requirements for survival, they won't have enough time to multiply and form large numbers of colonies. 2) Some types of bacteria might only reproduce very slowly compared to other organisms, which means you wouldn't see them growing much over short periods of time. Think about how long it takes plants to photosynthesize food - if your local ecosystem doesn't support plant life, no matter how many seeds you sow, none will ever reach maturity. Similarly, some microorganisms may take longer than others to produce offspring so slow reproduction rates mean fewer individuals are present at any given moment. 3) Maybe there's something preventing us from seeing larger-scale bacterial\n",
      "--------------------------------------------------\n",
      "Question 4: Why is it hard to focus/think after struggling with a difficult topic/situation?\n",
      "Model Output: Why is it hard to focus/think after struggling with a difficult topic/situation? You can get through the day, but you are not at your best.\n",
      "This happens because when we struggle in life or learn something new and challenging our brains go into \"fight mode\" which makes us feel anxious. We also have an innate fear of failure so this causes more anxiety even if there's no real risk involved (this was how I used to think about learning math). This means that while the brain is trying to process information, we're actually avoiding thinking about what could happen as well as feeling anxious due to all these other emotions running around inside us.\n",
      "----------\n",
      "Base Model Output: Why is it hard to focus/think after struggling with a difficult topic/situation? It's like your brain has been hijacked by the situation, and you can't get back on track.\n",
      "This happened when I was in college. My roommate got into my room one day (and not just any time), but during the night of an exam for our class. He had somehow managed to find out that we were taking the test at 9 am and he decided to crash his car around campus before driving me home so that way there would be no witnesses or anyone could report him missing. When I found out what he did, I felt completely overwhelmed - how do i think about this if I'm still trying to process everything going through my head?\n",
      "I know people say \"just calm down\" which doesn't really help... I feel guilty too much because I didn't see anything happening until afterwards and now everyone thinks something bad happened. And honestly even thinking about all these things makes me anxious again! How long does it take to recover from being traumatized by such a traumatic event without\n",
      "--------------------------------------------------\n",
      "Question 5: In the Middle Ages and Early Modern Era, if peasants were legally bound to their land, how did people manage to immigrate to cities or move?\n",
      "Model Output: In the Middle Ages and Early Modern Era, if peasants were legally bound to their land, how did people manage to immigrate to cities or move? They didn't have a passport, they couldn't pay taxes in advance, there was no national identity of any kind. What happened when these individuals crossed into urban areas where they had little legal recourse?\n",
      "\n",
      "I'm looking for an answer that will be applicable across Europe from around 1000-1800 CE, with some possible exceptions (the American colonies). I'd like something specific about England, but it might also apply to other parts of Europe as well.\n",
      "\n",
      "Thank you!\n",
      "----------\n",
      "Base Model Output: In the Middle Ages and Early Modern Era, if peasants were legally bound to their land, how did people manage to immigrate to cities or move? There are numerous examples of peasant migrations in Europe during this period. Here's a few I can think off right now: 1) The Black Death led to an increase in migration from rural areas as many left farms for more lucrative work opportunities (I'm thinking something like the Flanders region). This was partly due to famine that occurred around the same time leading to increased food prices. In fact it seems like most migrants went westward rather than north. How do you explain why they moved south instead? Why wasn't there mass movement into the countryside? Wasn't urbanization supposed to be driven by economic factors? 2) During times of war or famines, what happened with regards to these poor farmers who could not afford to pay rent on their land? Did those lands go to soldiers? Or other peasants? What about when lords took over land from nobles - would the former noble just get compensated for his loss of property or would he have some sort of rights? And finally,\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Question {i+1}: {sampled_data[i]}\")\n",
    "    print(f\"Model Output: {all_generated_texts[i]}\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"Base Model Output: {all_base_generated_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d79136d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "output_df_narrow = pd.DataFrame({\n",
    "    \"question\": sampled_data,\n",
    "    \"baseline_answer\": all_base_generated_texts,\n",
    "    \"fine_tuned_answer\": all_generated_texts\n",
    "})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df_narrow.to_csv(\"generated_answers_narrow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d461fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df_narrow=pd.read_csv(\"generated_answers_narrow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb87131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:01, 10.51it/s]/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.4527\n",
      "Average ROUGE-2 F1: 0.1911\n",
      "Average ROUGE-L F1: 0.2755\n",
      "Average BLEU Score: 0.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = evaluate_in_batches(\n",
    "    output_df_narrow['fine_tuned_answer'], output_df_narrow['baseline_answer'], batch_size=10\n",
    ")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda05c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : The difference between a learning disability and an intellectual disability\n",
      "Model Output: The difference between a learning disability and an intellectual disability is that the former refers to difficulties with processing information, whereas intelligence quotient (IQ) scores are used for determining if someone has above-average or below-average IQ. People who have learned disabilities can still learn new things just like anyone else; they simply may need accommodations in order to do so effectively.\n",
      "\n",
      "A person with a learning disability will typically require additional support during their education due to these challenges while being able to participate fully as well. In contrast, individuals with intellectual disabilities often rely on others because of their cognitive abilities but also possess unique strengths such as creativity, empathy, and strong social skills which make them valuable members of society.\n",
      "----------\n",
      "Base Model Output: The difference between a learning disability and an intellectual disability is the degree of intelligence. An individual with a developmental disorder has average to above-average abilities, while someone with an acquired brain injury or neurological condition may have below-average abilities.\n",
      "\n",
      "Learning disabilities are conditions that affect how you learn new information (e.g., dyslexia). You can be very smart but still struggle in school because your brain doesn't process certain types of information well. Intellectual disabilities result from physical or mental impairments affecting cognitive functions such as memory, concentration, problem-solving skills, communication, speech development, etc. They do not necessarily impact academic performance directly; rather they often require additional support services like therapy for children, vocational training programs for adults, and supportive living arrangements for older adults.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question=\"The difference between a learning disability and an intellectual disability\"\n",
    "inputs = tokenizer(text=question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Generate answers using the fine-tuned model\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "    base_outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "        # Decode the generated answers\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "base_generated_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "print(f\"Question : {question}\")\n",
    "print(f\"Model Output: {generated_text}\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"Base Model Output: {base_generated_text}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#propment engineering to enhanced the answers generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a703ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_question(question, context_type=\"history\", format_type=\"detailed\"):\n",
    "    # Context Setting based on topic type\n",
    "    if context_type == \"history\":\n",
    "        context = \"You are a knowledgeable historian. Provide a clear, detailed, and accurate answer.\"\n",
    "    elif context_type == \"science\":\n",
    "        context = \"You are an expert scientist. Provide a clear, detailed, and accurate explanation.\"\n",
    "    else:\n",
    "        context = \"You are an expert. Provide a clear, detailed, and accurate response.\"\n",
    "\n",
    "    # Construct the enhanced prompt with a simpler format\n",
    "    prompt = f\"Question: {question}\\nAnswer:\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Enhance the questions\n",
    "enhanced_questions = [enhance_question(q, context_type=\"history\" if \"history\" in q.lower() else \"science\") for q in sampled_data]\n",
    "\n",
    "# Generate responses with improved settings\n",
    "enhanced_responses = []\n",
    "for q in enhanced_questions:\n",
    "    # Tokenize the input with a higher max_length\n",
    "    inputs = tokenizer(q, return_tensors='pt', truncation=True, max_length=256).to(model.device)\n",
    "\n",
    "    # Generate the response with increased max_new_tokens and a clear stopping criterion\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id \n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output_tokens[0], skip_special_tokens=True).strip()\n",
    "    enhanced_responses.append(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2214f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_narrow['enhanced responses']=enhanced_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5789bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_narrow.to_csv(\"generated_answers_narrow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e100033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df_narrow_enhanced=pd.read_csv('generated_answers_narrow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b76fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>baseline_answer</th>\n",
       "      <th>fine_tuned_answer</th>\n",
       "      <th>enhanced responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I read a Reuters article today claiming that m...</td>\n",
       "      <td>I read a Reuters article today claiming that m...</td>\n",
       "      <td>I read a Reuters article today claiming that m...</td>\n",
       "      <td>Question: I read a Reuters article today claim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why are pedigree dogs considered by many to be...</td>\n",
       "      <td>Why are pedigree dogs considered by many to be...</td>\n",
       "      <td>Why are pedigree dogs considered by many to be...</td>\n",
       "      <td>Question: Why are pedigree dogs considered by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do photons act as the boson for electromag...</td>\n",
       "      <td>How do photons act as the boson for electromag...</td>\n",
       "      <td>How do photons act as the boson for electromag...</td>\n",
       "      <td>Question: How do photons act as the boson for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do officers in world war 2 use only a pist...</td>\n",
       "      <td>Why do officers in world war 2 use only a pist...</td>\n",
       "      <td>Why do officers in world war 2 use only a pist...</td>\n",
       "      <td>Question: Why do officers in world war 2 use o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is there any truth to the idea that the earlie...</td>\n",
       "      <td>Is there any truth to the idea that the earlie...</td>\n",
       "      <td>Is there any truth to the idea that the earlie...</td>\n",
       "      <td>Question: Is there any truth to the idea that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Anti-Aircraft Artillery Competitions?</td>\n",
       "      <td>Anti-Aircraft Artillery Competitions? What's t...</td>\n",
       "      <td>Anti-Aircraft Artillery Competitions? I know t...</td>\n",
       "      <td>Question: Anti-Aircraft Artillery Competitions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>When did smelting coins with the faces of stat...</td>\n",
       "      <td>When did smelting coins with the faces of stat...</td>\n",
       "      <td>When did smelting coins with the faces of stat...</td>\n",
       "      <td>Question: When did smelting coins with the fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>British Tax Exile of the 70s</td>\n",
       "      <td>British Tax Exile of the 70s\\nThe British gove...</td>\n",
       "      <td>British Tax Exile of the 70s: A Memoir by a Br...</td>\n",
       "      <td>Question: British Tax Exile of the 70s\\nAnswer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>What sort of institutions did the English leav...</td>\n",
       "      <td>What sort of institutions did the English leav...</td>\n",
       "      <td>What sort of institutions did the English leav...</td>\n",
       "      <td>Question: What sort of institutions did the En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Why does it seem like America chooses what's p...</td>\n",
       "      <td>Why does it seem like America chooses what's p...</td>\n",
       "      <td>Why does it seem like America chooses what's p...</td>\n",
       "      <td>Question: Why does it seem like America choose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    I read a Reuters article today claiming that m...   \n",
       "1    Why are pedigree dogs considered by many to be...   \n",
       "2    How do photons act as the boson for electromag...   \n",
       "3    Why do officers in world war 2 use only a pist...   \n",
       "4    Is there any truth to the idea that the earlie...   \n",
       "..                                                 ...   \n",
       "345              Anti-Aircraft Artillery Competitions?   \n",
       "346  When did smelting coins with the faces of stat...   \n",
       "347                       British Tax Exile of the 70s   \n",
       "348  What sort of institutions did the English leav...   \n",
       "349  Why does it seem like America chooses what's p...   \n",
       "\n",
       "                                       baseline_answer  \\\n",
       "0    I read a Reuters article today claiming that m...   \n",
       "1    Why are pedigree dogs considered by many to be...   \n",
       "2    How do photons act as the boson for electromag...   \n",
       "3    Why do officers in world war 2 use only a pist...   \n",
       "4    Is there any truth to the idea that the earlie...   \n",
       "..                                                 ...   \n",
       "345  Anti-Aircraft Artillery Competitions? What's t...   \n",
       "346  When did smelting coins with the faces of stat...   \n",
       "347  British Tax Exile of the 70s\\nThe British gove...   \n",
       "348  What sort of institutions did the English leav...   \n",
       "349  Why does it seem like America chooses what's p...   \n",
       "\n",
       "                                     fine_tuned_answer  \\\n",
       "0    I read a Reuters article today claiming that m...   \n",
       "1    Why are pedigree dogs considered by many to be...   \n",
       "2    How do photons act as the boson for electromag...   \n",
       "3    Why do officers in world war 2 use only a pist...   \n",
       "4    Is there any truth to the idea that the earlie...   \n",
       "..                                                 ...   \n",
       "345  Anti-Aircraft Artillery Competitions? I know t...   \n",
       "346  When did smelting coins with the faces of stat...   \n",
       "347  British Tax Exile of the 70s: A Memoir by a Br...   \n",
       "348  What sort of institutions did the English leav...   \n",
       "349  Why does it seem like America chooses what's p...   \n",
       "\n",
       "                                    enhanced responses  \n",
       "0    Question: I read a Reuters article today claim...  \n",
       "1    Question: Why are pedigree dogs considered by ...  \n",
       "2    Question: How do photons act as the boson for ...  \n",
       "3    Question: Why do officers in world war 2 use o...  \n",
       "4    Question: Is there any truth to the idea that ...  \n",
       "..                                                 ...  \n",
       "345  Question: Anti-Aircraft Artillery Competitions...  \n",
       "346  Question: When did smelting coins with the fac...  \n",
       "347  Question: British Tax Exile of the 70s\\nAnswer...  \n",
       "348  Question: What sort of institutions did the En...  \n",
       "349  Question: Why does it seem like America choose...  \n",
       "\n",
       "[350 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df_narrow_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a0dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:02,  9.99it/s]/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.4148\n",
      "Average ROUGE-2 F1: 0.1568\n",
      "Average ROUGE-L F1: 0.2403\n",
      "Average BLEU Score: 0.1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge1_scores, rouge2_scores, rougeL_scores, bleu_scores = evaluate_in_batches(\n",
    "    output_df_narrow_enhanced['enhanced responses'], output_df_narrow_enhanced['baseline_answer'], batch_size=10\n",
    ")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above ROUGE is not improved, therefore try to use support document to ehance performance. above code is testing only, \n",
    "#will use another notebook due to this one is slow now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5761cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list=filtered_full['question'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5359241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: elasticsearch in ./.local/lib/python3.9/site-packages (8.16.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in ./.local/lib/python3.9/site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: certifi in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143b05c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in ./.local/lib/python3.9/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.local/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd0a63e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ElasticsearchSearch\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b9adae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_top_10(question):\n",
    "    # Encode the question\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "\n",
    "    # Query Elasticsearch for top 100 results\n",
    "    response = es.search(\n",
    "        index=\"common_crawl_index\",\n",
    "        body={\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": question\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Extract top 100 URLs and text\n",
    "    results = [(hit[\"_source\"][\"url\"], hit[\"_source\"][\"text\"]) for hit in response[\"hits\"][\"hits\"]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ecb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df =pd.DataFrame(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fb2784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did humans start keeping track of their age?</td>\n",
       "      <td>This is more of a question for r/AskAnthropolo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How fast did news of political developments tr...</td>\n",
       "      <td>It would take about a month for official lette...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does rain change intensity when it falls e...</td>\n",
       "      <td>Over short periods like minutes, it just depen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If light and radio waves are both electromagne...</td>\n",
       "      <td>Well, you don't really need to \"illuminate\" th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the distribution of photon like?</td>\n",
       "      <td>It is simply that there are so many photons th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107328</th>\n",
       "      <td>Is it true that Goethe's book \"Die Leiden Des ...</td>\n",
       "      <td>Yes. It is true. It also coined the German ter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107329</th>\n",
       "      <td>Why is it hard to find a star system with 9 or...</td>\n",
       "      <td>If you take our solar system as an example it ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107330</th>\n",
       "      <td>What exactly does it mean for a territory to n...</td>\n",
       "      <td>They're like your dog. You totally consider hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107331</th>\n",
       "      <td>Would Varangians in early Kievan Rus have prac...</td>\n",
       "      <td>It is believed that the Varangians in the Rus ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107332</th>\n",
       "      <td>Why is California always a blue state? Why is ...</td>\n",
       "      <td>While large part of land are red areas, there ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107333 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "0       When did humans start keeping track of their age?   \n",
       "1       How fast did news of political developments tr...   \n",
       "2       How does rain change intensity when it falls e...   \n",
       "3       If light and radio waves are both electromagne...   \n",
       "4                What is the distribution of photon like?   \n",
       "...                                                   ...   \n",
       "107328  Is it true that Goethe's book \"Die Leiden Des ...   \n",
       "107329  Why is it hard to find a star system with 9 or...   \n",
       "107330  What exactly does it mean for a territory to n...   \n",
       "107331  Would Varangians in early Kievan Rus have prac...   \n",
       "107332  Why is California always a blue state? Why is ...   \n",
       "\n",
       "                                                   answer  category  labels  \n",
       "0       This is more of a question for r/AskAnthropolo...         0       0  \n",
       "1       It would take about a month for official lette...         0       0  \n",
       "2       Over short periods like minutes, it just depen...         1       1  \n",
       "3       Well, you don't really need to \"illuminate\" th...         1       1  \n",
       "4       It is simply that there are so many photons th...         1       1  \n",
       "...                                                   ...       ...     ...  \n",
       "107328  Yes. It is true. It also coined the German ter...         0       0  \n",
       "107329  If you take our solar system as an example it ...         1       1  \n",
       "107330  They're like your dog. You totally consider hi...         0       0  \n",
       "107331  It is believed that the Varangians in the Rus ...         0       0  \n",
       "107332  While large part of land are red areas, there ...         0       0  \n",
       "\n",
       "[107333 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ccd6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'category', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 107333\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231d5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset into train (80%) and validation (20%)\n",
    "dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Access the train and validation sets\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df80b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "label_mapping = {'history': 0, 'science': 1}\n",
    "# Load the model and specify the number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', num_labels=len(label_mapping)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ae1665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinmei/.local/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',       # Output directory\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',         # Log directory\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bc2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinmei/.local/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16101' max='16101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16101/16101 1:16:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.067644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.082582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16101, training_loss=0.04028868102174421, metrics={'train_runtime': 4577.819, 'train_samples_per_second': 56.271, 'train_steps_per_second': 3.517, 'total_flos': 3.412333695898829e+16, 'train_loss': 0.04028868102174421, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cafd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = trainer.model\n",
    "output_dir = \"C:/Users/luan/Desktop/peft_weights_crawl\"\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa70c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9999803304672241}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "#make a classifier to forecast the type of cotent of common crawl.\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = pipeline('text-classification', model='C:/Users/luan/Desktop/peft_weights_crawl', \n",
    "                      tokenizer='C:/Users/luan/Desktop/peft_weights_crawl',device=device)\n",
    "\n",
    "text = \"What are the causes of the American Revolution?\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a74859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
